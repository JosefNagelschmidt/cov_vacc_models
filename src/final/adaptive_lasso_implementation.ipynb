{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continued-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, chain\n",
    "\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from scipy import linalg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-greene",
   "metadata": {},
   "source": [
    "For an introduction to coordinate descent and reasons for making use of it in the lasso case, have a look at\n",
    "\n",
    "**Friedman, Jerome, et al. \"Pathwise coordinate optimization.\" Annals of applied statistics 1.2 (2007): 302-332**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-coating",
   "metadata": {},
   "source": [
    "The basic version of coordinate descent for lasso is quite straightforward to implement. However, there are many strategies and tricks to make the algorithm faster, and these\n",
    "do not only consist of software engineering and numerical techniques, but clever mathematical subsetting of active sets for variables etc. In this work, I make use of some of those methods, however I do not\n",
    "parallelize the execution of the base functions, such that for large numbers of regressors ($p$ large) the code cannot reach the speed of the scikit-learn or glmnet implementation. However, for moderate numbers\n",
    "of regressors, the performance is adequate and sufficient for my purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protected-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(X, y):\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape((len(y), 1))\n",
    "        \n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "    \n",
    "    return X_standardized, y_standardized, x_mean, x_std, y_mean, y_std\n",
    "\n",
    "def soft_threshold(rho, lamda, w):\n",
    "    \"\"\"Soft threshold function used for standardized data within the lasso regression\"\"\"\n",
    "    if rho < -lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho > lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "@njit\n",
    "def soft_threshold_numba(rho, lamda, w):\n",
    "    if rho < -lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho > lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_lamda_path_numba(X, y):\n",
    "    epsilon = 0.0001\n",
    "    K = 100\n",
    "    m, p = X.shape\n",
    "\n",
    "    y = y.reshape((m, 1))\n",
    "    sx = X\n",
    "    sy = y\n",
    "\n",
    "    lambda_max = np.max(np.abs(np.sum(sx * sy, axis=0))) / m\n",
    "    lamda_path = np.exp(\n",
    "        np.linspace(np.log(lambda_max), np.log(lambda_max * epsilon), np.int64(K))\n",
    "    )\n",
    "\n",
    "    return lamda_path\n",
    "\n",
    "@njit\n",
    "def count_non_zero_coeffs(theta_vec):\n",
    "    s = 0\n",
    "    for i in theta_vec:\n",
    "        if np.abs(i) > 1e-04:\n",
    "            s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-grant",
   "metadata": {},
   "source": [
    "#### Basic lasso implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-writer",
   "metadata": {},
   "source": [
    "Before building the adaptive lasso, we need a lasso implementation that supports taking weights for penalties for the regressors as an additional argument. \n",
    "Therefore, let us have a look at a very naive coordinate descent based implementation of the lasso function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conservative-pottery",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "):\n",
    "\n",
    "    m, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = m * get_lamda_path_numba(X=X, y=y)\n",
    "    else:\n",
    "        path = m * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / m\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            for j in range(p):\n",
    "                w_j = penalty_factors[j]\n",
    "                X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                y_pred = X @ theta\n",
    "                rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                z = np.sum(np.square(X_j))\n",
    "\n",
    "                if intercept:\n",
    "                    if j == 0:\n",
    "                        theta[j] = rho / z\n",
    "                    else:\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                else:\n",
    "                    theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-plasma",
   "metadata": {},
   "source": [
    "- This function takes the regressor matrix $X$, the dependent variable vector $y$, optionally a starting vector $\\theta$ for the linear coefficients, optionally the mentioned penalties on coefficients for later (adaptive lasso step) and whether and intercept should be fitted or not. Note that \"num_iters\" is a parameter for the ultimate stopping point of the algorithm. In general, coordinate descent converges much faster than in 100 updates, such that num_iters = 100 can be left as a default maximum update value.\n",
    "- After standardization of the inputs (necessary for lasso) in the first lines, a path of lamda penalties (which are the standard penalties on all coefficients, not the individual ones mentioned before) is calculated dependent on the data (following the strategy used in glmnet), if none was provided. The factor of *m* (sample size) in front of the lamda path is convenient to have consistent results (when using the same lambda values) with sci-kit learn and glmnet, since I used a slightly different version of the loss function (no sample size considered), such that my optimal lambdas would be different from their implementations (this does not change the result however, after cross-validation).\n",
    "- After initialization of start values for the coefficients that we would like to solve for, and optional penalties (adaptive lasso), we enter the main loops:\n",
    "* For each lambda we do num_iters iterations (no other stopping criterion implemented yet), and in each iteration we update all coefficients (p), one by one, via the famous soft threshold function.\n",
    "- Finally, since we standardized regressors and the dependent variable, in addition to the optimal coefficient vector theta_std on the standardized scale, I also return the optimal theta vector at the natural, i.e. original, scale by using basic arithmetic. One just has to be careful with the intercept (if it was provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-update",
   "metadata": {},
   "source": [
    "#### Adding a stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-cooler",
   "metadata": {},
   "source": [
    "- Let's move on to the first strategy to improve the speed of this method: Using a threshold as a stopping criterion.\n",
    "- In this case, we stop the second level loop before reaching num_iters, if the absolute difference between each updated theta and its old value is below some threshold- for all thetas individually (p in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "streaming-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_thresh_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "):\n",
    "\n",
    "    m, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = m * get_lamda_path_numba(X=X, y=y)\n",
    "    else:\n",
    "        path = m * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / m\n",
    "        tol_vals = np.full((p,), False)\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    w_j = penalty_factors[j]\n",
    "                    X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = rho / z\n",
    "                        else:\n",
    "                            if (\n",
    "                                np.abs(\n",
    "                                    theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                                )\n",
    "                                < thresh\n",
    "                            ):\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                    else:\n",
    "                        if (\n",
    "                            np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j))\n",
    "                            < thresh\n",
    "                        ):\n",
    "                            tol_vals[j] = True\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat\n",
    "        result.append(output)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-carnival",
   "metadata": {},
   "source": [
    "- As visible above, I just added a logical vector (filled with logical \"False\"s) that is updated each loop iteration, and the loop only continues conditional on the vector containing at least one \"False\".\n",
    "- The concern that an updated value could go from small updated deviations back to large ones (which would be a problem in the code above), is in general in coordinate descent not urgent, but will be dealt with in the implementations further below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-anniversary",
   "metadata": {},
   "source": [
    "#### Adding the \"warm start\" feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-graduate",
   "metadata": {},
   "source": [
    "The next logical step is to make use of *warm starts*: Since we are not only solving the minimization problem for one lambda value, but for a whole sequence, it makes sense not to start from the default initialization values of theta for each lambda again, but instead use the already quite nice theta solutions from the predecessor lambda as starting points for the next lambda in the sequence (the sequence is monotone). This is basically just adding two lines and one input argument for the function, since the old theta solutions are already in working memory, just above they were set back to default values before each new lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bound-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_thresh_lasso_warm_start(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "    \n",
    "    m, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = m * get_lamda_path_numba(X=X, y=y)\n",
    "    else:\n",
    "        path = m * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / m\n",
    "        tol_vals = np.full((p,), False)\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    w_j = penalty_factors[j]\n",
    "                    X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = rho / z\n",
    "                        else:\n",
    "                            if (\n",
    "                                np.abs(\n",
    "                                    theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                                )\n",
    "                                < thresh\n",
    "                            ):\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                    else:\n",
    "                        if (\n",
    "                            np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j))\n",
    "                            < thresh\n",
    "                        ):\n",
    "                            tol_vals[j] = True\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat\n",
    "        result.append(output)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-biodiversity",
   "metadata": {},
   "source": [
    "#### Introducing the focus on \"active sets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-breakfast",
   "metadata": {},
   "source": [
    "After an initial cycle through all *p* variables, one can restrict further iterations to the *active set* till convergence; and finally do one more cycle through all *p* to check if the active set has changed. This helps especially when $p \\gg N$. To make things easier to read, I have outsourced the update of the coefficients within each loop to an external function (below). Active in this context simply means not too close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "proud-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(\n",
    "    X, y, theta, active_set, penalty_factors, intercept, lamda, thresh, active_thresh\n",
    "):\n",
    "    active_set_converged_check = np.full((len(active_set),), False)\n",
    "    active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "    for subindex, j in enumerate(active_set):\n",
    "        w_j = penalty_factors[j]\n",
    "        X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "        y_pred = X @ theta\n",
    "        rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "        z = np.sum(np.square(X_j))\n",
    "\n",
    "        if intercept:\n",
    "            if j == 0:\n",
    "                tmp = rho / z\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] = tmp\n",
    "            else:\n",
    "                tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] = tmp\n",
    "\n",
    "        else:\n",
    "            tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            if np.abs(tmp) < active_thresh:\n",
    "                active_set_update[subindex] = False\n",
    "            if np.abs(theta[j] - tmp) < thresh:\n",
    "                active_set_converged_check[subindex] = True\n",
    "            theta[j] = tmp\n",
    "\n",
    "    active_set_converged = np.all(active_set_converged_check)\n",
    "    active_set = active_set[active_set_update]\n",
    "\n",
    "    return [theta, active_set, active_set_converged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vocal-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_set_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    active_thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "\n",
    "    m, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = m * get_lamda_path_numba(X=X, y=y)\n",
    "    else:\n",
    "        path = m * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / m\n",
    "        \n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                theta, active_set, active_set_converged = update_coeffs(\n",
    "                    X=X,\n",
    "                    y=y,\n",
    "                    theta=theta,\n",
    "                    active_set=active_set,\n",
    "                    penalty_factors=penalty_factors,\n",
    "                    intercept=intercept,\n",
    "                    lamda=lamda,\n",
    "                    thresh=thresh,\n",
    "                    active_thresh=active_thresh,\n",
    "                )\n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "                theta, active_set, active_set_converged = update_coeffs(\n",
    "                    X=X,\n",
    "                    y=y,\n",
    "                    theta=theta,\n",
    "                    active_set=active_set,\n",
    "                    penalty_factors=penalty_factors,\n",
    "                    intercept=intercept,\n",
    "                    lamda=lamda,\n",
    "                    thresh=thresh,\n",
    "                    active_thresh=active_thresh,\n",
    "                )\n",
    "\n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat\n",
    "        result.append(output)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-lindsay",
   "metadata": {},
   "source": [
    "#### Using **numba** to speed things up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-momentum",
   "metadata": {},
   "source": [
    "Since *numba* threw errors when outsourcing the update_coeffs as before (due to type problems when returning the updated parameters), I had to put everythin into one large numba function, and create loops for some parts that are not supported by numba yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rising-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def lasso_numba(\n",
    "    X,\n",
    "    y,\n",
    "    lamda_path=None,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    active_thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    x_mean = np.zeros((p,), dtype=np.float64)\n",
    "\n",
    "    for i in range(p):\n",
    "        x_mean[i] = X[:, i].mean()\n",
    "\n",
    "    x_std = np.zeros((p,), dtype=np.float64)\n",
    "\n",
    "    for i in range(p):\n",
    "        x_std[i] = X[:, i].std()\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "\n",
    "    if intercept:\n",
    "        X_tmp = np.ones((m, p + 1))\n",
    "        X_tmp[:, 1:] = X\n",
    "        X = X_tmp\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = m * get_lamda_path_numba(X=X_standardized, y=y_standardized)\n",
    "    else:\n",
    "        path = m * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X_tmp = np.ones((m, p + 1))\n",
    "        X_tmp[:, 1:] = X_standardized\n",
    "        X_standardized = X_tmp\n",
    "\n",
    "    m, p = X_standardized.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    lamdas = []\n",
    "    thetas = []\n",
    "    thetas_nat = []\n",
    "    BIC = []\n",
    "\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                active_set_converged_check = np.full((len(active_set),), False)\n",
    "                active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X_standardized @ theta\n",
    "\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X_standardized[obs, j].item() * (\n",
    "                            y_standardized[obs].item()\n",
    "                            - y_pred[obs].item()\n",
    "                            + theta[j].item() * X_standardized[obs, j].item()\n",
    "                        )\n",
    "                        z += np.square(X_standardized[obs, j].item())\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] = tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "\n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "\n",
    "                active_set_converged_check = np.full((len(active_set),), False)\n",
    "                active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "                m, p = X_standardized.shape\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X_standardized @ theta\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X_standardized[obs, j].item() * (\n",
    "                            y_standardized[obs].item()\n",
    "                            - y_pred[obs].item()\n",
    "                            + theta[j].item() * X_standardized[obs, j].item()\n",
    "                        )\n",
    "                        z += np.square(X_standardized[obs, j].item())\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] = tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "\n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_tmp = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_tmp = np.ones((p,))\n",
    "            theta_tmp[1:] = theta_betas\n",
    "            theta_tmp[0] = theta_0\n",
    "\n",
    "        m, p = X.shape\n",
    "        theta_bic = np.ones((p, 1))\n",
    "        theta_bic[:, 0] = theta_tmp\n",
    "        residuals_hat = np.sum(np.square(y - X @ theta_bic))\n",
    "        df_lamda = count_non_zero_coeffs(theta_vec=theta_bic.flatten())\n",
    "        BIC_lasso = residuals_hat / (m * y_std ** 2) + np.log(m) / m * df_lamda\n",
    "\n",
    "        lamdas.append(lamda / m)\n",
    "        thetas.append(np.copy(theta).flatten())\n",
    "        thetas_nat.append(theta_tmp)\n",
    "        BIC.append(BIC_lasso)\n",
    "\n",
    "    return lamdas, thetas, thetas_nat, BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-fifth",
   "metadata": {},
   "source": [
    "#### Performance comparison of the different implementations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-fitness",
   "metadata": {},
   "source": [
    "Let's do some small runtime benchmarking on the different versions above. There are several classes of combinations between $n$ and $p$ that are of interest, namely \n",
    "1. $p \\gg n$\n",
    "2. $n \\gg p$\n",
    "3. $n \\approx p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-discretion",
   "metadata": {},
   "source": [
    "We also add an scikit learn implementation for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satisfactory-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_learn_lasso(X, y, intercept=True, lamda_path=None):\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    X_standardized, y_standardized, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = get_lamda_path_numba(X=X_standardized, y=y_standardized)\n",
    "    else: \n",
    "        path = lamda_path\n",
    "\n",
    "    y_standardized = y_standardized.flatten()\n",
    "\n",
    "    lamdas = []\n",
    "    coeffs = []\n",
    "    \n",
    "    for lamda in path:\n",
    "        reg = Lasso(alpha= lamda, fit_intercept = intercept)\n",
    "        reg.fit(X_standardized, y_standardized)\n",
    "        \n",
    "        if intercept:\n",
    "            coef = np.insert(arr=reg.coef_, obj=0, values=reg.intercept_)\n",
    "        else:\n",
    "            coef = reg.coef_\n",
    "        \n",
    "        lamdas.append(lamda)\n",
    "        coeffs.append(np.copy(coef))\n",
    "\n",
    "    return lamdas, coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "armed-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 50\n",
    "\n",
    "methods = {\"naive_lasso\": naive_lasso,\n",
    "           \"eps_thresh_lasso\": eps_thresh_lasso, \n",
    "           \"eps_thresh_lasso_warm_start\": eps_thresh_lasso_warm_start,\n",
    "           \"active_set_lasso\": active_set_lasso,\n",
    "           \"lasso_numba\": lasso_numba,\n",
    "           \"sk_learn_lasso\": sk_learn_lasso\n",
    "          }\n",
    "\n",
    "selected_comb = [(2000, 20), (50, 50), (50, 200)]\n",
    "lasso_implementations = list(methods.keys())\n",
    "\n",
    "index = product(selected_comb, lasso_implementations)\n",
    "index = list(map(lambda idx: (idx[0][0], idx[0][1], idx[1]), index))\n",
    "\n",
    "index_pd = pd.MultiIndex.from_tuples(\n",
    "    index,\n",
    "    names=(\"n\", \"p\", \"method\"),\n",
    ")\n",
    "\n",
    "simulations = range(n_simulations)\n",
    "column_names = list(map(lambda sim: \"sim_number_\" + str(sim), simulations))\n",
    "df = pd.DataFrame(columns=column_names, \n",
    "                        index=index_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "angry-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(n, p):\n",
    "    X = np.random.rand(n, p)\n",
    "    y = np.array(3 * X[:,7] + X[:,3] + X[:,5] + X[:,6] + 4*X[:,11] + 4*X[:,8] -1.5 * X[:,0] -14.5*X[:,1] + 5 + np.random.normal(0,1,n), dtype=np.float64).reshape(-1,1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-credits",
   "metadata": {},
   "source": [
    "For replication you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "stuffed-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run with n_simulations = 50, taking approx. 1 hour on AMD Ryzen 9 5950X (16-Cores, 3.4 GHz no overclocking)\n",
    "# instead you can use two cells below to read in the my results\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    for n_val, p_val, implementation in index:\n",
    "        \n",
    "        X, y = get_test_data(n=n_val, p=p_val)\n",
    "\n",
    "        start = timer()\n",
    "        res = methods[implementation](X=X, y=y)\n",
    "        end = timer()\n",
    "        time = end - start\n",
    "\n",
    "        index_df = (n_val, p_val, implementation)\n",
    "        df.at[index_df, \"sim_number_\" + str(i)] = time\n",
    "        \n",
    "df = df.apply(pd.to_numeric)\n",
    "df['mean'] = df.drop(['sim_number_0'], axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-pressing",
   "metadata": {},
   "source": [
    "Or just read in the results from my simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unlike-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adaptive_lasso_imp_performance_overview_all.csv\", index_col=[\"n\", \"p\", \"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunrise-cowboy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">2000</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">20</th>\n",
       "      <th>naive_lasso</th>\n",
       "      <td>8.308077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso</th>\n",
       "      <td>0.576845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso_warm_start</th>\n",
       "      <td>0.425290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_set_lasso</th>\n",
       "      <td>0.362742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>0.112011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.054290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">50</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">50</th>\n",
       "      <th>naive_lasso</th>\n",
       "      <td>6.378891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso</th>\n",
       "      <td>5.807401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso_warm_start</th>\n",
       "      <td>5.566837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_set_lasso</th>\n",
       "      <td>3.841453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>0.100913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.052910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">200</th>\n",
       "      <th>naive_lasso</th>\n",
       "      <td>28.496357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso</th>\n",
       "      <td>28.178992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps_thresh_lasso_warm_start</th>\n",
       "      <td>26.595705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_set_lasso</th>\n",
       "      <td>6.008584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>0.627181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.269916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           mean\n",
       "n    p   method                                \n",
       "2000 20  naive_lasso                   8.308077\n",
       "         eps_thresh_lasso              0.576845\n",
       "         eps_thresh_lasso_warm_start   0.425290\n",
       "         active_set_lasso              0.362742\n",
       "         lasso_numba                   0.112011\n",
       "         sk_learn_lasso                0.054290\n",
       "50   50  naive_lasso                   6.378891\n",
       "         eps_thresh_lasso              5.807401\n",
       "         eps_thresh_lasso_warm_start   5.566837\n",
       "         active_set_lasso              3.841453\n",
       "         lasso_numba                   0.100913\n",
       "         sk_learn_lasso                0.052910\n",
       "     200 naive_lasso                  28.496357\n",
       "         eps_thresh_lasso             28.178992\n",
       "         eps_thresh_lasso_warm_start  26.595705\n",
       "         active_set_lasso              6.008584\n",
       "         lasso_numba                   0.627181\n",
       "         sk_learn_lasso                0.269916"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-contamination",
   "metadata": {},
   "source": [
    "- Since some of the methods performed pretty slow, I had to set $p$ small enough. For a more detailed comparison between my best performing implementation, namely *lasso_numba* with the one given in *sk_learn_lasso*, see below (fixed $n$, increasing $p$). \n",
    "- However, it is already clear from the results above that the usage of active sets in *active_set_lasso*, as well as the just-in-time compiled *numba* setup make out the biggest improvements in performance.\n",
    "- Additionaly, for relatively small and moderate sizes in $p$ ($n$ is not really relevant for performance issues in this algorithm) the runtimes of my *lasso_numba* implementation are within the same order of magnitude (twice the runtime) as the *sk_learn_lasso*, which is surprisingly good (since *sk_learn_lasso* has been optimized for years, using cython and parallelization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daily-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations_opt = 25\n",
    "\n",
    "methods_opt = {\"lasso_numba\": lasso_numba,\n",
    "               \"sk_learn_lasso\": sk_learn_lasso\n",
    "              }\n",
    "n = [2000]\n",
    "p = np.arange(start=100, stop=1100, step=100)\n",
    "lasso_implementations = list(methods_opt.keys())\n",
    "\n",
    "index = list(product(n, p , lasso_implementations))\n",
    "\n",
    "index_pd = pd.MultiIndex.from_tuples(\n",
    "    index,\n",
    "    names=(\"n\", \"p\", \"method\"),\n",
    ")\n",
    "\n",
    "simulations = range(n_simulations_opt)\n",
    "column_names = list(map(lambda sim: \"sim_number_\" + str(sim), simulations))\n",
    "\n",
    "df_opt = pd.DataFrame(columns=column_names, \n",
    "                        index=index_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unable-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_simulations_opt):\n",
    "    for n, p, implementation in index:\n",
    "        X, y = get_test_data(n=n, p=p)\n",
    "        start = timer()\n",
    "        res = methods_opt[implementation](X=X, y=y)\n",
    "        end = timer()\n",
    "        time = end - start\n",
    "\n",
    "        index_df = (n, p, implementation)\n",
    "        df_opt.at[index_df, \"sim_number_\" + str(i)] = time\n",
    "\n",
    "df_opt = df_opt.apply(pd.to_numeric)\n",
    "df_opt['mean'] = df_opt.drop(['sim_number_0'], axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "charitable-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt = pd.read_csv(\"adaptive_lasso_imp_performance_overivew_best_two.csv\", index_col=[\"n\", \"p\", \"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "derived-filling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">2000</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">100</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>0.528449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.138793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">200</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>1.368613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.252778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">300</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>2.604982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.358237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">400</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>4.565362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.506065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">500</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>7.463895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.619391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">600</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>11.257669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.758889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">700</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>16.860213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>0.969172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">800</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>22.897318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>1.163743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">900</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>32.524605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>1.401938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>lasso_numba</th>\n",
       "      <td>44.901259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_learn_lasso</th>\n",
       "      <td>1.639592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mean\n",
       "n    p    method                   \n",
       "2000 100  lasso_numba      0.528449\n",
       "          sk_learn_lasso   0.138793\n",
       "     200  lasso_numba      1.368613\n",
       "          sk_learn_lasso   0.252778\n",
       "     300  lasso_numba      2.604982\n",
       "          sk_learn_lasso   0.358237\n",
       "     400  lasso_numba      4.565362\n",
       "          sk_learn_lasso   0.506065\n",
       "     500  lasso_numba      7.463895\n",
       "          sk_learn_lasso   0.619391\n",
       "     600  lasso_numba     11.257669\n",
       "          sk_learn_lasso   0.758889\n",
       "     700  lasso_numba     16.860213\n",
       "          sk_learn_lasso   0.969172\n",
       "     800  lasso_numba     22.897318\n",
       "          sk_learn_lasso   1.163743\n",
       "     900  lasso_numba     32.524605\n",
       "          sk_learn_lasso   1.401938\n",
       "     1000 lasso_numba     44.901259\n",
       "          sk_learn_lasso   1.639592"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_opt[['mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "periodic-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='p', ylabel='mean'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seaborn.lineplot(data=df_opt.reset_index().drop([\"n\"], axis=1), x=\"p\", y=\"mean\", hue=\"method\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
