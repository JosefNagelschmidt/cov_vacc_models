{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lined-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.linalg import toeplitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "increasing-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath(\"../..\") + \"/bld/analysis/sparse_modelling_adaptive_lasso_add_profession_yes_add_political_yes_january_no.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "instrumental-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "certified-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = np.array([False, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acting-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = np.array([0.0, 1, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adaptive-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"supp\": supp, \"theta_hat\": theta_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "written-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.array([[0.1, 1.1],[0.4,1.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cognitive-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretable_confidence_intervals(adaptive_lasso_tuned_obj, intercept):\n",
    "\n",
    "    if intercept:\n",
    "        theta_opt_nat = np.delete(arr=adaptive_lasso_tuned_obj[\"theta_opt_nat\"], obj=0)\n",
    "    else:\n",
    "        theta_opt_nat = adaptive_lasso_tuned_obj[\"theta_opt_nat\"]\n",
    "        \n",
    "    lower_conf_bound = np.full([len(adaptive_lasso_tuned_obj[\"selected_support\"])], np.nan)\n",
    "    upper_conf_bound = np.full([len(adaptive_lasso_tuned_obj[\"selected_support\"])], np.nan)\n",
    "\n",
    "    np.put(a=lower_conf_bound, ind=np.nonzero(adaptive_lasso_tuned_obj[\"selected_support\"])[0], v=adaptive_lasso_tuned_obj[\"conf_intervals_nat\"][:,0])\n",
    "    np.put(a=upper_conf_bound, ind=np.nonzero(adaptive_lasso_tuned_obj[\"selected_support\"])[0], v=adaptive_lasso_tuned_obj[\"conf_intervals_nat\"][:,1])\n",
    "    \n",
    "    d = {\"selected_support\": adaptive_lasso_tuned_obj[\"selected_support\"], \"theta_nat\": theta_opt_nat, \"lower_conf_bound\": lower_conf_bound, \"upper_conf_bound\": upper_conf_bound}\n",
    "\n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "thousand-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = interpretable_confidence_intervals(adaptive_lasso_tuned_obj=res, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "central-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_support</th>\n",
       "      <th>theta_nat</th>\n",
       "      <th>lower_conf_bound</th>\n",
       "      <th>upper_conf_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.543573</td>\n",
       "      <td>-1.735907</td>\n",
       "      <td>-1.351238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>-14.517524</td>\n",
       "      <td>-14.719569</td>\n",
       "      <td>-14.315479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.922498</td>\n",
       "      <td>0.716573</td>\n",
       "      <td>1.128422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>1.191439</td>\n",
       "      <td>0.985579</td>\n",
       "      <td>1.397298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>1.077859</td>\n",
       "      <td>0.876208</td>\n",
       "      <td>1.279510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>4.062946</td>\n",
       "      <td>3.861060</td>\n",
       "      <td>4.264833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>4.044491</td>\n",
       "      <td>3.840583</td>\n",
       "      <td>4.248399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    selected_support  theta_nat  lower_conf_bound  upper_conf_bound\n",
       "0               True  -1.543573         -1.735907         -1.351238\n",
       "1               True -14.517524        -14.719569        -14.315479\n",
       "2              False   0.000000               NaN               NaN\n",
       "3               True   0.922498          0.716573          1.128422\n",
       "4              False   0.000000               NaN               NaN\n",
       "5               True   1.191439          0.985579          1.397298\n",
       "6               True   1.077859          0.876208          1.279510\n",
       "7               True   4.062946          3.861060          4.264833\n",
       "8               True   4.044491          3.840583          4.248399\n",
       "9              False   0.000000               NaN               NaN\n",
       "10             False   0.000000               NaN               NaN\n",
       "11             False   0.000000               NaN               NaN\n",
       "12             False   0.000000               NaN               NaN\n",
       "13             False   0.000000               NaN               NaN\n",
       "14             False   0.000000               NaN               NaN"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "powered-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = adaptive_lasso_tuned(X=X, y=y, first_stage=\"OLS\", intercept=False, cross_valid_split = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "touched-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selected_support': array([False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False]),\n",
       " 'theta_opt_nat': array([  0.        , -14.47916171,   0.        ,   0.93312674,\n",
       "          0.        ,   1.27730202,   1.10002887,   4.1561386 ,\n",
       "          4.03912775,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ]),\n",
       " 'theta_opt_std': array([ 0.        , -0.90061153,  0.        ,  0.05644397,  0.        ,\n",
       "         0.0796895 ,  0.06917909,  0.25309979,  0.25210765,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " 'conf_intervals_nat': array([[-15.31959262, -13.63873079],\n",
       "        [  0.08299445,   1.78325902],\n",
       "        [  0.4162978 ,   2.13830625],\n",
       "        [  0.26324876,   1.93680897],\n",
       "        [  3.31569867,   4.99657854],\n",
       "        [  3.19385276,   4.88440274]]),\n",
       " 'conf_intervals_std': array([[-0.91542198, -0.88580108],\n",
       "        [ 0.04158548,  0.07130246],\n",
       "        [ 0.06481313,  0.09456586],\n",
       "        [ 0.05429053,  0.08406766],\n",
       "        [ 0.23829358,  0.267906  ],\n",
       "        [ 0.23727726,  0.26693804]])}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "binary-month",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_support</th>\n",
       "      <th>theta_nat</th>\n",
       "      <th>lower_conf_bound</th>\n",
       "      <th>upper_conf_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.543573</td>\n",
       "      <td>-1.735907</td>\n",
       "      <td>-1.351238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>-14.517524</td>\n",
       "      <td>-14.719569</td>\n",
       "      <td>-14.315479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.922498</td>\n",
       "      <td>0.716573</td>\n",
       "      <td>1.128422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>1.191439</td>\n",
       "      <td>0.985579</td>\n",
       "      <td>1.397298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>1.077859</td>\n",
       "      <td>0.876208</td>\n",
       "      <td>1.279510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>4.062946</td>\n",
       "      <td>3.861060</td>\n",
       "      <td>4.264833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>4.044491</td>\n",
       "      <td>3.840583</td>\n",
       "      <td>4.248399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    selected_support  theta_nat  lower_conf_bound  upper_conf_bound\n",
       "0               True  -1.543573         -1.735907         -1.351238\n",
       "1               True -14.517524        -14.719569        -14.315479\n",
       "2              False   0.000000               NaN               NaN\n",
       "3               True   0.922498          0.716573          1.128422\n",
       "4              False   0.000000               NaN               NaN\n",
       "5               True   1.191439          0.985579          1.397298\n",
       "6               True   1.077859          0.876208          1.279510\n",
       "7               True   4.062946          3.861060          4.264833\n",
       "8               True   4.044491          3.840583          4.248399\n",
       "9              False   0.000000               NaN               NaN\n",
       "10             False   0.000000               NaN               NaN\n",
       "11             False   0.000000               NaN               NaN\n",
       "12             False   0.000000               NaN               NaN\n",
       "13             False   0.000000               NaN               NaN\n",
       "14             False   0.000000               NaN               NaN"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df, d2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sharing-guide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 1.1, nan, 1.4])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_conf_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "armed-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conf_lower_bound'] = np.where(df['supp'] == True, conf[, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "guided-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sexual-temple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supp</th>\n",
       "      <th>theta_hat</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    supp  theta_hat  conf\n",
       "0  False        0.0  -1.0\n",
       "1   True        1.0   3.0\n",
       "2  False        0.0  -1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "comic-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath(\"../..\") + \"/bld/data/sparse_modelling_df_add_profession_yes_add_political_yes.csv\")\n",
    "df2 = pd.read_csv(os.path.abspath(\"../..\") + \"/bld/data/sparse_modelling_df_add_profession_no_add_political_no.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "collective-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data_dgp(rel_path, january=True):\n",
    "    data = pd.read_csv(rel_path)\n",
    "    data = data.drop([\"personal_id\"], axis=1)\n",
    "    \n",
    "    if january:\n",
    "        y =  data[[\"vaccine_intention_jan\"]]\n",
    "        X = data.drop([\"vaccine_intention_jan\", \"vaccine_intention_jul\"], axis=1)\n",
    "    else:\n",
    "        y =  data[[\"vaccine_intention_jul\"]]\n",
    "        X = data.drop([\"vaccine_intention_jan\", \"vaccine_intention_jul\"], axis=1)\n",
    "    \n",
    "    y_numpy = y.to_numpy()\n",
    "    X_numpy = X.to_numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    y_std = scaler.fit_transform(y_numpy)\n",
    "    X_std = scaler.fit_transform(X_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "waiting-least",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2515, 91)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def generate_moderate_correlation(n):\n",
    "\n",
    "    np.random.seed(seed=8)\n",
    "    eigvals = np.random.uniform(low=0.01, high=1.0, size=n)\n",
    "\n",
    "    mean_val = np.mean(eigvals)\n",
    "    half = int(n/2)\n",
    "    threefourth = int(half + half / 2)\n",
    "\n",
    "    eigvals_normalized = eigvals / mean_val\n",
    "    sort = np.sort(eigvals_normalized)\n",
    "\n",
    "    gain = 5/6 * np.sum(sort[0:(half)])\n",
    "\n",
    "    sort[0:half] = sort[0:(half)] / 6\n",
    "    sort[half:n] = sort[(half):n] + gain / half\n",
    "\n",
    "    gain2 = 7/8 * np.sum(sort[half:(threefourth)])\n",
    "\n",
    "    sort[half:threefourth] = sort[half:(threefourth)] / 8\n",
    "    sort[threefourth:n] = sort[(threefourth):n] + gain2 / (n - threefourth)\n",
    "\n",
    "    correlation_matrix = random_correlation.rvs(sort)\n",
    "\n",
    "    cov_matrix = corr2cov(correlation_matrix, np.random.uniform(low=0.3, high=2.0, size=n))\n",
    "    return cov_matrix \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "virgin-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_link(X, beta):\n",
    "    return X @ beta\n",
    "\n",
    "def polynomial_link(X, beta):\n",
    "    n, p = X.shape\n",
    "    \n",
    "    linear_effect = X[:,0].reshape((n,1)) @ beta[0,:].reshape((1,1))\n",
    "    residual = X[:,1:] @ beta[1:,:].reshape((p-1,1))\n",
    "    \n",
    "    return linear_effect + (residual)**3 + (residual)**2 + (residual) + 1\n",
    "\n",
    "def sine_link(X, beta):\n",
    "    n, p = X.shape\n",
    "    \n",
    "    linear_effect = X[:,0].reshape((n,1)) @ beta[0,:].reshape((1,1))\n",
    "    residual = X[:,1:] @ beta[1:,:].reshape((p-1,1))\n",
    "    \n",
    "    return linear_effect + np.sin(3*residual) + np.sin(2*residual) + np.sin(residual) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "flexible-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_mat(p, identity):\n",
    "    if identity:\n",
    "        return np.identity(p)\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        \n",
    "def get_X_mat(n, p, identity_cov, seed):\n",
    "    assert p > 10\n",
    "    \n",
    "    np.random.seed(seed=1)\n",
    "    \n",
    "    if identity_cov:\n",
    "        return np.random.multivariate_normal(mean=np.repeat(0, repeats=p), cov=get_cov_mat(p=p, identity=identity_cov), size= n)\n",
    "    else:\n",
    "        return np.random.multivariate_normal(mean=np.repeat(0, repeats=p), cov=get_cov_mat(p=p, identity=identity_cov), size = n)\n",
    "    \n",
    "def get_true_beta_vec(p):\n",
    "    assert p > 10\n",
    "    return np.insert(arr=np.zeros((p - 10,)), obj=0, values=np.array([  1.,   0.5,   3.,  -6., -0.1,  11.,  -6., -0.05,  -9.5,  6.])).reshape((p,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "associate-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgp(n, p, link_function, identity_cov=True):\n",
    "    \n",
    "    X = get_X_mat(n=n, p=p, identity_cov=True, seed=1)\n",
    "    beta_vec = get_true_beta_vec(p=p)\n",
    "    \n",
    "    y = link_function(X, beta_vec).flatten() + np.random.normal(loc=0.0, scale=1.0, size=n)\n",
    "    \n",
    "    return {\"X\": X, \"y\": y, \"beta\": beta_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "short-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dgp(n, p, identity_cov=True):\n",
    "    X = get_X_mat(n=n, p=p, identity_cov=identity_cov, seed=1)\n",
    "    beta_vec = get_true_beta_vec(p=p)\n",
    "    \n",
    "    y = (X @ beta_vec).flatten() + np.random.normal(loc=0.0, scale=1.0, size=n)\n",
    "    \n",
    "    return {\"X\": X, \"y\": y, \"beta\": beta_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "baking-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = linear_dgp(n=12, p=11, identity_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "urban-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = get_dgp(n=12, p=11, link_function=linear_link, identity_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "raised-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = get_true_beta_vec(p=11)\n",
    "X = get_X_mat(n=12, p=11, identity_cov=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "streaming-lying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ],\n",
       "       [ 0.5 ],\n",
       "       [ 3.  ],\n",
       "       [-6.  ],\n",
       "       [-0.1 ],\n",
       "       [11.  ],\n",
       "       [-6.  ],\n",
       "       [-0.05],\n",
       "       [-9.5 ],\n",
       "       [ 6.  ],\n",
       "       [ 0.  ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "intermediate-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "n = 3000\n",
    "X = np.random.rand(n,15)\n",
    "#y = np.array(13* X[:,7] + X[:,3] + X[:,5] + X[:,6] +X[:,7] +X[:,8]- 1.5 * X[:,0] - 14.5 * X[:,1] + 5 + np.random.normal(0,1,n), dtype=np.float64).reshape(-1,1)\n",
    "y = np.array(3* X[:,7] + X[:,3] + X[:,5] + X[:,6] +X[:,7] + 4* X[:,8]- 1.5 * X[:,0] - 14.5 * X[:,1] + 5 + np.random.normal(0,1,n), dtype=np.float64).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "peripheral-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-bunch",
   "metadata": {},
   "source": [
    "1 -> 1.5\n",
    "2 -> -14.5\n",
    "3 -> 0\n",
    "4 -> 1\n",
    "5 -> 0\n",
    "6 -> 1\n",
    "7 -> 1\n",
    "8 -> 3\n",
    "9 -> 4\n",
    "rest -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "streaming-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.54000e+01, 1.33238e+04, 5.90000e+00, 2.71000e+01, 2.00000e-01,\n",
       "       7.00000e-01, 1.92000e+01, 2.38800e+02, 1.92900e+02, 0.00000e+00,\n",
       "       3.10000e+00, 1.00000e-01, 0.00000e+00, 2.00000e-01, 3.10000e+00])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n , p = X.shape\n",
    "selector = SelectKBest(f_regression, k= int(p/10))\n",
    "selector.fit(X, y.flatten())\n",
    "selector.scores_.round(1)\n",
    "#scores = -np.log10(selector.pvalues_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "outside-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dutch-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = X.shape\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators= 500, max_depth=6, random_state=0)\n",
    "regr = regr.fit(X, y.flatten())\n",
    "model = SelectFromModel(regr, prefit=True, threshold=-np.inf, max_features=int(p/4))\n",
    "X_new = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "chief-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 15)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "occupied-music",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "commercial-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.17022005e-01, 7.20324493e-01, 1.14374817e-04, 3.02332573e-01,\n",
       "       1.46755891e-01, 9.23385948e-02, 1.86260211e-01, 3.45560727e-01,\n",
       "       3.96767474e-01, 5.38816734e-01, 4.19194514e-01, 6.85219500e-01,\n",
       "       2.04452250e-01, 8.78117436e-01, 2.73875932e-02])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "disabled-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from scipy import linalg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def standardize_input(X, y):\n",
    "    \"\"\"Standardizes the regressor matrix and the dependent vector.\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1) or (n, )\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple containing:\n",
    "\n",
    "            **X_standardized** (*np.ndarray*): standardized regressor matrix of shape (n, p) \\n\n",
    "            **y_standardized** (*np.ndarray*): standardized vector of the dependent variable *y*, of shape (n, 1) or (n, ) \\n\n",
    "            **x_mean** (*np.ndarray*): column means of the regressor matrix of shape (p, ) \\n\n",
    "            **x_std** (*np.ndarray*): column standard deviations of the regressor matrix of shape (p, ) \\n\n",
    "            **y_mean** (*np.float64*): mean of the vector of the dependent variable *y* \\n\n",
    "            **y_std** (*np.float64*): standard deviation of the vector of the dependent variable *y*\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape((len(y), 1))\n",
    "        \n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "    \n",
    "    return X_standardized, y_standardized, x_mean, x_std, y_mean, y_std\n",
    "\n",
    "\n",
    "@njit\n",
    "def count_non_zero_coeffs(theta_vec):\n",
    "\n",
    "    \"\"\"Determines the cardinality of the non-zero elements of a given input vector *theta_vec*.\n",
    "\n",
    "    Args:\n",
    "        theta_vec (np.ndarray): 1d array (p, ) representation of a coefficient vector\n",
    "\n",
    "    Returns:\n",
    "        **s** (*int*): cardinality of the non-zero elements of *theta_vec*\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    s = 0\n",
    "    for i in theta_vec:\n",
    "        if np.abs(i) > 1e-04:\n",
    "            s += 1\n",
    "    return s\n",
    "\n",
    "\n",
    "def soft_threshold(rho, lamda, w):\n",
    "\n",
    "    \"\"\"Soft threshold function used for standardized data within the lasso regression.\n",
    "\n",
    "    Args:\n",
    "        rho (float): defined as :math:`\\\\rho := X_j^T (y - y_{pred} + \\\\theta_j \\\\cdot X_j)`, where :math:`X_j` is the *j*-th column of the regressor matrix, *y* is the dependent variable vector, :math:`y_{pred} := X \\\\cdot \\\\theta` (projection), and :math:`\\\\theta` the coefficient vector.\n",
    "        lamda (float): non-negative regularization parameter in the l1-penalty term of the lasso\n",
    "        w (float): weight for a given coefficient within the l1-penalty term of the adaptive lasso\n",
    "\n",
    "    Returns:\n",
    "        (*float*): proximal mapping of the l1-norm; solution of coordinate descent for the update step in lasso\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if rho < -lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho > lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "@njit\n",
    "def soft_threshold_numba(rho, lamda, w):\n",
    "\n",
    "    \"\"\"Just-in-time compiled version of the *soft_threshold* function used within the lasso regression\"\"\"\n",
    "\n",
    "    if rho < -lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho > lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_lamda_path_numba(X_std, y_std):\n",
    "\n",
    "    \"\"\"Just-in-time compiled version of the *get_lamda_path* function used within the lasso regression\"\"\"\n",
    "\n",
    "    epsilon = 0.0001\n",
    "    K = 100\n",
    "\n",
    "    n, p = X_std.shape\n",
    "\n",
    "    y_std = y_std.reshape((n, 1))\n",
    "\n",
    "    if 0.5 * n <= p:\n",
    "        epsilon = 0.01\n",
    "\n",
    "    lambda_max = np.max(np.abs(np.sum(X_std * y_std, axis=0))) / n\n",
    "    lamda_path = np.exp(\n",
    "        np.linspace(np.log(lambda_max), np.log(lambda_max * epsilon), np.int64(K))\n",
    "    )\n",
    "\n",
    "    return lamda_path\n",
    "\n",
    "\n",
    "def get_lamda_path(X_std, y_std, epsilon=0.0001, K=100):\n",
    "\n",
    "    \"\"\"Calculates a data-dependent sequence of lambdas, for which we want to solve the lasso optimization problem. This approach follows\n",
    "    the one used in *glmnet* package in R.\n",
    "\n",
    "    Args:\n",
    "        X_std (np.ndarray): standardized regressor matrix of shape (n, p)\n",
    "        y_std (np.ndarray): standardized vector of the dependent variable *y*, of shape (n, 1) or (n, )\n",
    "        epsilon (float): parameter determining the lower bound of the lambda sequence\n",
    "        K (int): parameter determining the number of elements within the lambda sequence\n",
    "\n",
    "    Returns:\n",
    "        **lamda_path** (*np.ndarray*): data-dependent sequence of lambdas (optimization path for lasso)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X_std.shape\n",
    "\n",
    "    y_std = y_std.reshape((n, 1))\n",
    "\n",
    "    # to ensure that matrix inversion is stable in case that p > n\n",
    "    if 0.5 * n <= p:\n",
    "        epsilon = 0.01\n",
    "\n",
    "    # data-dependent part from glmnet\n",
    "    lambda_max = np.max(np.abs(np.sum(X_std * y_std, axis=0))) / n\n",
    "\n",
    "    # transformation such that we get many lambda elements close to zero, and a few large ones\n",
    "    lamda_path = np.exp(\n",
    "        np.linspace(\n",
    "            start=np.log(lambda_max), stop=np.log(lambda_max * epsilon), num=np.int64(K)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return lamda_path\n",
    "\n",
    "\n",
    "def update_coeffs(\n",
    "    X_std, y_std, theta, active_set, penalty_factors, intercept, lamda, thresh, active_thresh\n",
    "):\n",
    "    \"\"\"Calculates the update of the coefficients within each loop of *active_set_lasso*.\n",
    "\n",
    "    Args:\n",
    "        X_std (np.ndarray): standardized regressor matrix of shape (n, p)\n",
    "        y_std (np.ndarray): standardized vector of the dependent variable *y*, of shape (n, 1)\n",
    "        theta (np.ndarray): vector of coefficients of shape (p, 1)\n",
    "        active_set (np.ndarray): indeces of coefficients to consider in the update, i.e. these coefficients are not zero and still active\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1)\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "        lamda (float): non-negative regularization parameter in the l1-penalty term of the lasso\n",
    "        thresh (float): threshold for determining whether the update was small enough to classify the coefficient as converged\n",
    "        active_thresh (float): threshold for determining whether the coefficient is still different enough from zero to be considered active\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple containing:\n",
    "\n",
    "            **theta** (*np.ndarray*): updated vector of coefficients of shape (p, 1) \\n\n",
    "            **active_set** (*np.ndarray*): indeces of coefficients to consider in the next cycle, i.e. updated version of *active_set* \\n\n",
    "            **active_set_converged** (*bool*): Logical value whether all ex ante active coefficients have converged within this cycle\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # set up two logical vectors for the active_set classication later\n",
    "    active_set_converged_check = np.full((len(active_set),), False)\n",
    "    active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "    # main update step\n",
    "    for subindex, j in enumerate(active_set):\n",
    "        w_j = penalty_factors[j]\n",
    "        X_j = X_std[:, j].reshape(-1, 1)\n",
    "\n",
    "        y_pred = X_std @ theta\n",
    "        rho = X_j.T @ (y_std - y_pred + theta[j] * X_j)\n",
    "        z = np.sum(np.square(X_j))\n",
    "\n",
    "        if intercept:\n",
    "            if j == 0:\n",
    "                tmp = rho / z\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] = tmp\n",
    "            else:\n",
    "                tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] = tmp\n",
    "\n",
    "        else:\n",
    "            tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            if np.abs(tmp) < active_thresh:\n",
    "                active_set_update[subindex] = False\n",
    "            if np.abs(theta[j] - tmp) < thresh:\n",
    "                active_set_converged_check[subindex] = True\n",
    "            theta[j] = tmp\n",
    "\n",
    "    # test whether all ex ante active coefficients have converged within this cycle\n",
    "    active_set_converged = np.all(active_set_converged_check)\n",
    "\n",
    "    # remove coefficients from the active set that were too close to zero\n",
    "    active_set = active_set[active_set_update]\n",
    "\n",
    "    return theta, active_set, active_set_converged\n",
    "\n",
    "\n",
    "def naive_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "):\n",
    "    \"\"\"Naive coordinate descent implementation of the basic lasso optimization problem without stopping criterion except the maximum number of iterations.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1). If none are provided, the function defaults to providing a vector of ones, which is the standard lasso version.\n",
    "        theta (np.ndarray): initial starting values for the vector of coefficients of shape (p, 1). If none are provided, the function defaults to setting each coefficient to zero initially.\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default.\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100.\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "\n",
    "    Returns:\n",
    "        list: list of dicts, each containing:\n",
    "\n",
    "            **lamda** (*float*): non-negative regularization parameter in the l1-penalty term of the lasso and element of lamda_path \\n\n",
    "            **theta_std** (*np.ndarray*): optimal lasso coefficients on the standardized scale for the given lambda in the dictionary, shape (p, ) \\n\n",
    "            **theta_nat** (*np.ndarray*): optimal lasso coefficients on the original scale for the given lambda in the dictionary, shape (p, )\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = n * get_lamda_path_numba(X_std=X, y_std=y)\n",
    "    else:\n",
    "        path = n * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / n\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            for j in range(p):\n",
    "                w_j = penalty_factors[j]\n",
    "                X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                y_pred = X @ theta\n",
    "                rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                z = np.sum(np.square(X_j))\n",
    "\n",
    "                if intercept:\n",
    "                    if j == 0:\n",
    "                        theta[j] = rho / z\n",
    "                    else:\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                else:\n",
    "                    theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat\n",
    "        result.append(output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def eps_thresh_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "):\n",
    "    \"\"\"Improved coordinate descent implementation of the basic lasso optimization problem with threshold as stopping criterion.\n",
    "    Cycling is stopped if the absolute difference between each updated theta and its former value is below the threshold *thresh*.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1). If none are provided, the function defaults to providing a vector of ones, which is the standard lasso version.\n",
    "        theta (np.ndarray): initial starting values for the vector of coefficients of shape (p, 1). If none are provided, the function defaults to setting each coefficient to zero initially.\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default.\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100.\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "        thresh (float): determines the relevant threshold for the  absolute difference between each updated theta and its former value\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        list: list of dicts, each containing:\n",
    " \n",
    "            **lamda** (*float*): non-negative regularization parameter in the l1-penalty term of the lasso and element of lamda_path \\n\n",
    "            **theta_std** (*np.ndarray*): optimal lasso coefficients on the standardized scale for the given lambda in the dictionary, shape (p, ) \\n\n",
    "            **theta_nat** (*np.ndarray*): optimal lasso coefficients on the original scale for the given lambda in the dictionary, shape (p, )\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = n * get_lamda_path_numba(X_std=X, y_std=y)\n",
    "    else:\n",
    "        path = n * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / n\n",
    "        tol_vals = np.full((p,), False)\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    w_j = penalty_factors[j]\n",
    "                    X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = rho / z\n",
    "                        else:\n",
    "                            if (\n",
    "                                np.abs(\n",
    "                                    theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                                )\n",
    "                                < thresh\n",
    "                            ):\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                    else:\n",
    "                        if (\n",
    "                            np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j))\n",
    "                            < thresh\n",
    "                        ):\n",
    "                            tol_vals[j] = True\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat.flatten()\n",
    "        result.append(output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def eps_thresh_lasso_warm_start(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "    \"\"\"Further improved coordinate descent implementation of the basic lasso optimization problem with threshold as stopping criterion and the usage of *warm_starts*.\n",
    "    Cycling is stopped if the absolute difference between each updated theta and its former value is below the threshold *thresh*, and warm starts reuse the previously learned optimal coefficients as starting values for theta\n",
    "    in the optimization for the next lambda element in the *lamda_path*.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1). If none are provided, the function defaults to providing a vector of ones, which is the standard lasso version.\n",
    "        theta (np.ndarray): initial starting values for the vector of coefficients of shape (p, 1). If none are provided, the function defaults to setting each coefficient to zero initially.\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default.\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100.\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "        thresh (float): threshold for determining whether the update was small enough to classify the coefficient as converged\n",
    "        warm_start (bool): Logical value determining whether the *warm_starts* feature should be used.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        list: list of dicts, each containing:\n",
    "\n",
    "            **lamda** (*float*): non-negative regularization parameter in the l1-penalty term of the lasso and element of lamda_path \\n\n",
    "            **theta_std** (*np.ndarray*): optimal lasso coefficients on the standardized scale for the given lambda in the dictionary, shape (p, ) \\n\n",
    "            **theta_nat** (*np.ndarray*): optimal lasso coefficients on the original scale for the given lambda in the dictionary, shape (p, )\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = n * get_lamda_path_numba(X_std=X, y_std=y)\n",
    "    else:\n",
    "        path = n * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / n\n",
    "        tol_vals = np.full((p,), False)\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    w_j = penalty_factors[j]\n",
    "                    X_j = X[:, j].reshape(-1, 1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred + theta[j] * X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = rho / z\n",
    "                        else:\n",
    "                            if (\n",
    "                                np.abs(\n",
    "                                    theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                                )\n",
    "                                < thresh\n",
    "                            ):\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "\n",
    "                    else:\n",
    "                        if (\n",
    "                            np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j))\n",
    "                            < thresh\n",
    "                        ):\n",
    "                            tol_vals[j] = True\n",
    "                        theta[j] = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat.flatten()\n",
    "        result.append(output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def active_set_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    lamda_path=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    active_thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "    \"\"\"Even more improved coordinate descent implementation of the lasso optimization problem with threshold as stopping criterion, the usage of *warm_starts*, and the usage of *active sets*.\n",
    "    Cycling is stopped if the absolute difference between each updated theta and its former value is below the threshold *thresh*, and warm starts reuses the previously learned optimal coefficients as starting values for theta\n",
    "    in the optimization for the next lambda element in the *lamda_path*.\n",
    "    After an initial cycle through all *p* variables, the *active_set* feature restricts further iterations to the *active set* till convergence; and finally does one more cycle through all *p* variables to check if the active set has changed. This helps especially when *p* is large.\n",
    "    Uses the base function *update_coeffs* for readability.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1). If none are provided, the function defaults to providing a vector of ones, which is the standard lasso version\n",
    "        theta (np.ndarray): initial starting values for the vector of coefficients of shape (p, 1). If none are provided, the function defaults to setting each coefficient to zero initially\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "        thresh (float): threshold for determining whether the update was small enough to classify the coefficient as converged\n",
    "        active_thresh (float): threshold for determining whether the coefficient is still different enough from zero to be considered active\n",
    "        warm_start (bool): Logical value determining whether the *warm_starts* feature should be used\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        list: list of dicts, each containing:\n",
    "\n",
    "            **lamda** (*float*): non-negative regularization parameter in the l1-penalty term of the lasso and element of lamda_path \\n\n",
    "            **theta_std** (*np.ndarray*): optimal lasso coefficients on the standardized scale for the given lambda in the dictionary, shape (p, ) \\n\n",
    "            **theta_nat** (*np.ndarray*): optimal lasso coefficients on the original scale for the given lambda in the dictionary, shape (p, )\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "    X, y, x_mean, x_std, y_mean, y_std = standardize_input(X=X, y=y)\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = n * get_lamda_path_numba(X_std=X, y_std=y)\n",
    "    else:\n",
    "        path = n * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        output = {}\n",
    "        output[\"lamda\"] = lamda / n\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                theta, active_set, active_set_converged = update_coeffs(\n",
    "                    X_std=X,\n",
    "                    y_std=y,\n",
    "                    theta=theta,\n",
    "                    active_set=active_set,\n",
    "                    penalty_factors=penalty_factors,\n",
    "                    intercept=intercept,\n",
    "                    lamda=lamda,\n",
    "                    thresh=thresh,\n",
    "                    active_thresh=active_thresh,\n",
    "                )\n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "                theta, active_set, active_set_converged = update_coeffs(\n",
    "                    X_std=X,\n",
    "                    y_std=y,\n",
    "                    theta=theta,\n",
    "                    active_set=active_set,\n",
    "                    penalty_factors=penalty_factors,\n",
    "                    intercept=intercept,\n",
    "                    lamda=lamda,\n",
    "                    thresh=thresh,\n",
    "                    active_thresh=active_thresh,\n",
    "                )\n",
    "\n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr=theta_betas, obj=0, values=theta_0)\n",
    "\n",
    "        output[\"theta_std\"] = theta.flatten()\n",
    "        output[\"theta_nat\"] = theta_nat.flatten()\n",
    "        result.append(output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@njit\n",
    "def lasso_numba(\n",
    "    X,\n",
    "    y,\n",
    "    lamda_path=None,\n",
    "    penalty_factors=None,\n",
    "    theta=None,\n",
    "    num_iters=100,\n",
    "    intercept=True,\n",
    "    thresh=1e-7,\n",
    "    active_thresh=1e-7,\n",
    "    warm_start=True,\n",
    "):\n",
    "    \"\"\"Just-in-time compiled version of the *active_set_lasso* function.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        penalty_factors (np.ndarray): vector of penalties that function as weights for the coefficients within the l1-penalty term of the adaptive lasso; later used in the *soft_threshold* function. The shape must be (p, 1). If none are provided, the function defaults to providing a vector of ones, which is the standard lasso version\n",
    "        theta (np.ndarray): initial starting values for the vector of coefficients of shape (p, 1). If none are provided, the function defaults to setting each coefficient to zero initially\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting (adaptive) lasso\n",
    "        thresh (float): threshold for determining whether the update was small enough to classify the coefficient as converged\n",
    "        active_thresh (float): threshold for determining whether the coefficient is still different enough from zero to be considered active\n",
    "        warm_start (bool): Logical value determining whether the *warm_starts* feature should be used\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple containing:\n",
    "\n",
    "            **lamdas** (*list*): list of lamdas in lamda_path for which the lasso problem has been solved \\n\n",
    "            **thetas** (*list*): list of lasso coefficient vectors on the standardized scale, for each lambda in *lamdas* one optimal coefficient vector \\n\n",
    "            **thetas_nat** (*list*): list of lasso coefficient vectors on the original scale, for each lambda in *lamdas* one optimal coefficient vector \\n\n",
    "            **BIC** (*list*): list of BIC values, one for each lambda in *lamdas* (BIC calculated from the respective model trained given a lambda value). For details see **Zou, H., Hastie, T., & Tibshirani, R. (2007)**\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    x_mean = np.zeros((p,), dtype=np.float64)\n",
    "\n",
    "    for i in range(p):\n",
    "        x_mean[i] = X[:, i].mean()\n",
    "\n",
    "    x_std = np.zeros((p,), dtype=np.float64)\n",
    "\n",
    "    for i in range(p):\n",
    "        x_std[i] = X[:, i].std()\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "\n",
    "    if intercept:\n",
    "        X_tmp = np.ones((n, p + 1))\n",
    "        X_tmp[:, 1:] = X\n",
    "        X = X_tmp\n",
    "\n",
    "    if lamda_path is None:\n",
    "        path = n * get_lamda_path_numba(X_std=X_standardized, y_std=y_standardized)\n",
    "    else:\n",
    "        path = n * lamda_path\n",
    "\n",
    "    if intercept:\n",
    "        X_tmp = np.ones((n, p + 1))\n",
    "        X_tmp[:, 1:] = X_standardized\n",
    "        X_standardized = X_tmp\n",
    "\n",
    "    n, p = X_standardized.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p, 1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p, 1))\n",
    "\n",
    "    lamdas = []\n",
    "    thetas = []\n",
    "    thetas_nat = []\n",
    "    BIC = []\n",
    "\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p, 1))\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "\n",
    "        for _i in range(num_iters):\n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                active_set_converged_check = np.full((len(active_set),), False)\n",
    "                active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X_standardized @ theta\n",
    "\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(n):\n",
    "                        rho += X_standardized[obs, j].item() * (\n",
    "                            y_standardized[obs].item()\n",
    "                            - y_pred[obs].item()\n",
    "                            + theta[j].item() * X_standardized[obs, j].item()\n",
    "                        )\n",
    "                        z += np.square(X_standardized[obs, j].item())\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] = tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "\n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "\n",
    "                active_set_converged_check = np.full((len(active_set),), False)\n",
    "                active_set_update = np.full((len(active_set),), True)\n",
    "\n",
    "                n, p = X_standardized.shape\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X_standardized @ theta\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(n):\n",
    "                        rho += X_standardized[obs, j].item() * (\n",
    "                            y_standardized[obs].item()\n",
    "                            - y_pred[obs].item()\n",
    "                            + theta[j].item() * X_standardized[obs, j].item()\n",
    "                        )\n",
    "                        z += np.square(X_standardized[obs, j].item())\n",
    "\n",
    "                    if intercept:\n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] = tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] = tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "\n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not intercept:\n",
    "            theta_tmp = theta.flatten() / x_std * y_std\n",
    "        if intercept:\n",
    "            theta_0 = (\n",
    "                theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])\n",
    "            ) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_tmp = np.ones((p,))\n",
    "            theta_tmp[1:] = theta_betas\n",
    "            theta_tmp[0] = theta_0\n",
    "\n",
    "        n, p = X.shape\n",
    "        theta_bic = np.ones((p, 1))\n",
    "        theta_bic[:, 0] = theta_tmp\n",
    "        residuals_hat = np.sum(np.square(y - X @ theta_bic))\n",
    "        df_lamda = count_non_zero_coeffs(theta_vec=theta_bic.flatten())\n",
    "        BIC_lasso = residuals_hat / (n * y_std ** 2) + np.log(n) / n * df_lamda\n",
    "\n",
    "        lamdas.append(lamda / n)\n",
    "        thetas.append(np.copy(theta).flatten())\n",
    "        thetas_nat.append(theta_tmp)\n",
    "        BIC.append(BIC_lasso)\n",
    "\n",
    "    return lamdas, thetas, thetas_nat, BIC\n",
    "\n",
    "\n",
    "def adaptive_lasso(\n",
    "    X,\n",
    "    y,\n",
    "    intercept=True,\n",
    "    lamda_path=None,\n",
    "    gamma_path=None,\n",
    "    first_stage=\"OLS\",\n",
    "    num_iters=100,\n",
    "    out_as_df=True,\n",
    "):\n",
    "\n",
    "    \"\"\"Basic implementation of the adaptive lasso from **Zou, H. (2006)**.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        intercept (bool): logical value whether an intercept should be used when fitting lasso or OLS\n",
    "        lamda_path (np.ndarray): sequence of lambda values to solve the lasso problem for. If none are provided, the function provides a data-dependent sequence by default\n",
    "        gamma_path (np.ndarray): sequence of gamma values to solve the lasso problem for (see paper above for more details on gamma). If none are provided, the function provides a simple yet broad enough sequence by default\n",
    "        first_stage (str): Options are \"OLS\" and \"Lasso\" currently. Determines which method should be used for getting initial first-stage estimates for the coefficient vector. Defaults to \"OLS\". If \"Lasso\" is chosen, the full *lamda_path* is calculated and the lamda that minimzes BIC is taken as a final estimate (for selection consistency), following **Zou, H., Hastie, T., & Tibshirani, R. (2007)**\n",
    "        num_iters (int): Maximum number of cycles to update the coefficients; usually convergence is reached in under 100 iterations. Defaults to 100\n",
    "        out_as_df (bool): Logical value determining whether the output should be in pd.DataFrame format instead of lists. This is necessary for later use with the *cv_adaptive_lasso* function\n",
    "\n",
    "    Returns:\n",
    "        tuple: if out_as_df = False, tuple containing:\n",
    "  \n",
    "            **path_gamma** (*np.ndarray*): sequence of gamma values for which the lasso problem has actually been solved \\n\n",
    "            **results** (*list*): list of tuples returned by calls to \"lasso_numba\", one tuple for each gamma in *path_gamma* \\n\n",
    "            **weight_path** (*list*): list of np.ndarrays, each consisting of the coefficient weights used for solving lasso (for each gamma one)\n",
    "\n",
    "        if out_as_df = True, pd.DataFrame: \n",
    "            **df** (*pd.DataFrame*): dataframe of results, consisting of the *path_gamma* and *path_lamda* vectors as its multiindex; and the standardized coefficient vectors, original scaled coefficient vectors, as well as gamma_weight vectors as its cell entries.\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if gamma_path is None:\n",
    "        path_gamma = np.array([0.1, 0.5, 1, 2, 3, 4, 6, 8])\n",
    "    else:\n",
    "        path_gamma = gamma_path\n",
    "\n",
    "    if first_stage == \"OLS\":\n",
    "        reg = LinearRegression(fit_intercept=intercept).fit(X, y)\n",
    "        coeffs = reg.coef_.T\n",
    "    elif first_stage == \"Lasso\":\n",
    "        res = lasso_numba(X=X, y=y)\n",
    "\n",
    "        # taking the lasso fit that minimizes the BIC estimate\n",
    "        index_lamda_opt = np.where(res[3] == np.amin(res[3]))[0][0]\n",
    "        # remove the intercept, since it should not be penalized\n",
    "        coeffs = np.delete(res[1][index_lamda_opt], 0).reshape((p, 1))\n",
    "\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            \"This feature has so far only been implemented for OLS and Lasso as its first-stage estimators.\"\n",
    "        )\n",
    "\n",
    "    # avoiding numerical issues, division by zero etc.\n",
    "    coeffs[np.abs(coeffs) < 1.00e-15] = 1.00e-15\n",
    "\n",
    "    results = []\n",
    "    weight_path = []\n",
    "\n",
    "    # this is the second stage, making use of the first-stage estimates from before saved in \"coeffs\"\n",
    "    for gamma in path_gamma:\n",
    "\n",
    "        if intercept:\n",
    "            weights = np.ones((p + 1, 1))\n",
    "            weights[1:, :] = 1.0 / np.abs(coeffs) ** gamma\n",
    "        else:\n",
    "            weights = 1.0 / np.abs(coeffs) ** gamma\n",
    "\n",
    "        res = lasso_numba(\n",
    "            X,\n",
    "            y,\n",
    "            lamda_path=lamda_path,\n",
    "            penalty_factors=weights,\n",
    "            theta=None,\n",
    "            num_iters=num_iters,\n",
    "            intercept=intercept,\n",
    "            thresh=1e-7,\n",
    "            active_thresh=1e-7,\n",
    "            warm_start=True,\n",
    "        )\n",
    "\n",
    "        weight_path.append(weights)\n",
    "        results.append(res)\n",
    "\n",
    "    if out_as_df:\n",
    "        lamda_p = results[0][0]\n",
    "        df = pd.DataFrame(\n",
    "            list(product(path_gamma, lamda_p)), columns=[\"gamma\", \"lamda\"]\n",
    "        )\n",
    "        df[\"theta_std\"] = np.nan\n",
    "        df[\"theta_nat\"] = np.nan\n",
    "        df[\"gamma_weights\"] = np.nan\n",
    "        df = df.astype(object)\n",
    "        df = df.set_index([\"gamma\", \"lamda\"])\n",
    "\n",
    "        for id_gamma, gamma in enumerate(path_gamma):\n",
    "            for idx, lamda in enumerate(results[id_gamma][0]):\n",
    "                index = (gamma, lamda)\n",
    "                df.at[index, \"theta_std\"] = results[id_gamma][1][idx]\n",
    "                df.at[index, \"theta_nat\"] = results[id_gamma][2][idx]\n",
    "                df.at[index, \"gamma_weights\"] = weight_path[id_gamma]\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        return path_gamma, results, weight_path\n",
    "\n",
    "\n",
    "def get_conf_intervals(\n",
    "    lamda, weights, theta_std, theta_nat, X, X_std, intercept, y, y_std\n",
    "):\n",
    "\n",
    "    \"\"\"Calculates the adaptive lasso confidence intervals of active coefficients, i.e. coefficients in *theta_std* that are distinct from zero.\n",
    "    The calculations are based on the Standard Error Formula in chapter 3.6. in Zou, H. (2006).\n",
    "\n",
    "    Args:\n",
    "        lamda (float): non-negative regularization parameter in the l1-penalty term of the lasso\n",
    "        weights (np.ndarray): vector of penalties/ weights for the coefficients within the l1-penalty term of the adaptive lasso. The shape must be (p, 1)\n",
    "        theta_std (np.ndarray): vector of previously fitted adaptive lasso coefficients on standardized scale of shape (p, )\n",
    "        theta_nat (np.ndarray): vector of previously fitted adaptive lasso coefficients on original scale of shape (p, )\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        X_std (np.ndarray): standardized regressor matrix of shape (n, p)\n",
    "        intercept (bool): logical value whether an intercept was used while fitting the adaptive lasso\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        y_std (np.ndarray): standardized vector of the dependent variable *y*, of shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "        dict: dict containing:\n",
    "\n",
    "            **beta_hat_nat_cov_mat** (*np.ndarray*): estimated asymptotic covariance matrix (on original scale) for the active set of estimated coefficients from adaptive lasso \\n\n",
    "            **beta_hat_std_cov_mat** (*np.ndarray*): estimated asymptotic covariance matrix (on standardized scale) for the active set of estimated coefficients from adaptive lasso \\n\n",
    "            **conf_intervals_nat** (*np.ndarray*): elementwise confidence intervals at the 95% confidence level for coefficients in the active set, on original scale \\n\n",
    "            **conf_intervals_std** (*np.ndarray*): elementwise confidence intervals at the 95% confidence level for coefficients in the active set, on standardized scale \\n\n",
    "            **active_set** (*np.ndarray*): the active (non-zero) set of coefficients in *theta_nat* for which confidence intervals were calculated\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "    if intercept:\n",
    "        X_with_intercept = np.insert(X, 0, 1, axis=1)\n",
    "        X_std_with_intercept = np.insert(X_std, 0, 1, axis=1)\n",
    "\n",
    "        sigma_hat_nat = (\n",
    "            1\n",
    "            / n\n",
    "            * np.sum(\n",
    "                np.square(y - X_with_intercept @ theta_nat.reshape((len(theta_nat), 1)))\n",
    "            )\n",
    "        )\n",
    "        sigma_hat_std = (\n",
    "            1\n",
    "            / n\n",
    "            * np.sum(\n",
    "                np.square(\n",
    "                    y_std\n",
    "                    - X_std_with_intercept @ theta_std.reshape((len(theta_std), 1))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        sigma_hat_nat = (\n",
    "            1 / n * np.sum(np.square(y - X @ theta_nat.reshape((len(theta_nat), 1))))\n",
    "        )\n",
    "        sigma_hat_std = (\n",
    "            1\n",
    "            / n\n",
    "            * np.sum(np.square(y_std - X_std @ theta_std.reshape((len(theta_std), 1))))\n",
    "        )\n",
    "\n",
    "    if intercept:\n",
    "        theta_std = np.delete(arr=theta_std, obj=0)\n",
    "        theta_nat = np.delete(arr=theta_nat, obj=0)\n",
    "        weights = np.delete(arr=weights, obj=0, axis=0)\n",
    "\n",
    "    weights = weights.flatten()\n",
    "\n",
    "    # selection of the relevant (\"active\") columns in the regressor matrix X and in the coefficient vectors\n",
    "    active_set = np.invert(np.isclose(np.zeros(p), theta_nat, atol=1e-06))\n",
    "\n",
    "    X_active = X[:, active_set]\n",
    "    X_std_active = X_std[:, active_set]\n",
    "    theta_nat_active = theta_nat[active_set]\n",
    "    theta_std_active = theta_std[active_set]\n",
    "    weights_active = weights[active_set]\n",
    "\n",
    "    # the steps below follow the Standard Error Formula in chapter 3.6. in Zou, H. (2006)\n",
    "    diag_std = weights_active / theta_std_active\n",
    "    diag_nat = weights_active / theta_nat_active\n",
    "\n",
    "    sigma_beta_std = np.diag(v=diag_std, k=0)\n",
    "    sigma_beta_nat = np.diag(v=diag_nat, k=0)\n",
    "\n",
    "    main_mat_nat = X_active.T @ X_active + lamda * sigma_beta_nat\n",
    "    main_mat_std = X_std_active.T @ X_std_active + lamda * sigma_beta_std\n",
    "\n",
    "    main_mat_nat_inverse = linalg.inv(main_mat_nat)\n",
    "    main_mat_std_inverse = linalg.inv(main_mat_std)\n",
    "\n",
    "    beta_hat_nat_cov_mat = sigma_hat_nat * (\n",
    "        main_mat_nat_inverse @ X_active.T @ X_active @ main_mat_nat_inverse\n",
    "    )\n",
    "    beta_hat_std_cov_mat = sigma_hat_std * (\n",
    "        main_mat_std_inverse @ X_std_active.T @ X_std_active @ main_mat_std_inverse\n",
    "    )\n",
    "\n",
    "    conf_intervals_nat_upper_bound = theta_nat_active + 1.96 * np.sqrt(\n",
    "        np.diag(beta_hat_nat_cov_mat)\n",
    "    )\n",
    "    conf_intervals_nat_lower_bound = theta_nat_active - 1.96 * np.sqrt(\n",
    "        np.diag(beta_hat_nat_cov_mat)\n",
    "    )\n",
    "\n",
    "    conf_intervals_nat = np.column_stack(\n",
    "        (conf_intervals_nat_lower_bound, conf_intervals_nat_upper_bound)\n",
    "    )\n",
    "\n",
    "    conf_intervals_std_upper_bound = theta_std_active + 1.96 * np.sqrt(\n",
    "        np.diag(beta_hat_std_cov_mat)\n",
    "    )\n",
    "    conf_intervals_std_lower_bound = theta_std_active - 1.96 * np.sqrt(\n",
    "        np.diag(beta_hat_std_cov_mat)\n",
    "    )\n",
    "\n",
    "    conf_intervals_std = np.column_stack(\n",
    "        (conf_intervals_std_lower_bound, conf_intervals_std_upper_bound)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"beta_hat_nat_cov_mat\": beta_hat_nat_cov_mat,\n",
    "        \"beta_hat_std_cov_mat\": beta_hat_std_cov_mat,\n",
    "        \"conf_intervals_nat\": conf_intervals_nat,\n",
    "        \"conf_intervals_std\": conf_intervals_std,\n",
    "        \"active_set\": active_set,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_prediction(X, y, theta_nat, intercept=True):\n",
    "\n",
    "    \"\"\"Helper function that takes fitted coefficients from a lasso problem and outputs fitted values on some sample data matrix X (possibly not the same as the one used for fitting). It also\n",
    "    calculates the mean-squared-error between fitted and actual responses *y_hat* and *y*.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor (data) matrix of shape (n, p), not necessarily the same matrix that was used for fitting *theta_nat*\n",
    "        y (np.ndarray): corresponding vector of the dependent variable *y*, of shape (n, 1)\n",
    "        theta_nat (np.ndarray): vector of previously fitted adaptive lasso coefficients on original scale of shape (p, )\n",
    "        intercept (bool): logical value whether an intercept was used while fitting the adaptive lasso for *theta_nat*\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple containing:\n",
    "\n",
    "            **y_hat** (*np.ndarray*): fitted responses for the given sample in *X* \\n\n",
    "            **mse** (*float*): mean-squared-error between fitted and actual responses *y_hat* and *y*\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if intercept:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    y_hat = X @ theta_nat.reshape((X.shape[1], 1))\n",
    "    mse = np.sum(np.square(y - y_hat))\n",
    "    return y_hat, mse\n",
    "\n",
    "\n",
    "def cv_adaptive_lasso(X, y, intercept=True, first_stage=\"OLS\", cross_valid_split = True):\n",
    "\n",
    "    \"\"\"Helper function that cross-validates the adaptive lasso in the dimensions (gamma, lambda). Two folds are used in the current implementation.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        intercept (bool): logical value whether an intercept shall be used while fitting the adaptive lasso for *theta_nat*\n",
    "        first_stage (str): Options are \"OLS\" and \"Lasso\" currently. Determines which method should be used for getting initial first-stage estimates for the coefficient vector. Defaults to \"OLS\". If \"Lasso\" is chosen, the full *lamda_path* is calculated and the lamda that minimzes BIC is taken as a final estimate (for selection consistency), following **Zou, H., Hastie, T., & Tibshirani, R. (2007)**\n",
    "        cross_valid_split (bool): Option to turn-off the exchange of the two cross-validation folds, thus evaluation is only done once on the second fold, while training is done on the first fold. In small samples this is currently necessary, due to issues with float multiindexing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: tuple containing:\n",
    "\n",
    "            **cv_overview** (*pd.DataFrame*): Summary of all the possible combinations of gammas and lambdas with corresponding model performances in the differnt folds \\n\n",
    "            **params_opt** (*tuple*): Optimal tuple of (gamma_opt, lambda_opt), measured in mean-squared error loss. Element of *cv_overview*.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "\n",
    "    indices = np.random.permutation(n)\n",
    "    fold_1_idx, fold_2_idx = indices[: int(n / 2)], indices[int(n / 2) :]\n",
    "    X_fold_1, X_fold_2 = X[fold_1_idx, :], X[fold_2_idx, :]\n",
    "    y_fold_1, y_fold_2 = y[fold_1_idx, :], y[fold_2_idx, :]\n",
    "\n",
    "    gamma_path = np.array([0.5, 1, 2, 3, 4, 6, 8, 10])\n",
    "    lamda_path = get_lamda_path(X_std=X_standardized, y_std=y_standardized)\n",
    "\n",
    "    trained_on_fold_1 = adaptive_lasso(\n",
    "        X=X_fold_1,\n",
    "        y=y_fold_1,\n",
    "        intercept=intercept,\n",
    "        lamda_path=lamda_path,\n",
    "        gamma_path=gamma_path,\n",
    "        first_stage=first_stage,\n",
    "        num_iters=100,\n",
    "        out_as_df=True,\n",
    "    )\n",
    "\n",
    "    if cross_valid_split:\n",
    "        trained_on_fold_2 = adaptive_lasso(\n",
    "            X=X_fold_2,\n",
    "            y=y_fold_2,\n",
    "            intercept=intercept,\n",
    "            lamda_path=lamda_path,\n",
    "            gamma_path=gamma_path,\n",
    "            first_stage=first_stage,\n",
    "            num_iters=100,\n",
    "            out_as_df=True,\n",
    "        )\n",
    "\n",
    "        trained_on_fold_1[\"mse_1\"] = np.nan\n",
    "        trained_on_fold_2[\"mse_2\"] = np.nan\n",
    "\n",
    "        prod = product(\n",
    "            trained_on_fold_1.index.get_level_values(\"gamma\").unique(),\n",
    "            trained_on_fold_1.index.get_level_values(\"lamda\").unique(),\n",
    "        )\n",
    "        for gamma, lamda in prod:\n",
    "            index = (gamma, lamda)\n",
    "            y_hat_1, mse_1 = make_prediction(\n",
    "                X=X_fold_2,\n",
    "                y=y_fold_2,\n",
    "                theta_nat=trained_on_fold_1.at[index, \"theta_nat\"],\n",
    "                intercept=intercept,\n",
    "            )\n",
    "\n",
    "            y_hat_2, mse_2 = make_prediction(\n",
    "                X=X_fold_1,\n",
    "                y=y_fold_1,\n",
    "                theta_nat=trained_on_fold_2.at[index, \"theta_nat\"],\n",
    "                intercept=intercept,\n",
    "            )\n",
    "\n",
    "            trained_on_fold_1.at[index, \"mse_1\"] = mse_1\n",
    "            trained_on_fold_2.at[index, \"mse_2\"] = mse_2\n",
    "\n",
    "        cv_overview = trained_on_fold_1.merge(\n",
    "            trained_on_fold_2, how=\"left\", on=[\"gamma\", \"lamda\"]\n",
    "        )[[\"mse_1\", \"mse_2\"]]\n",
    "        cv_overview[\"mean_mse\"] = cv_overview.mean(axis=1)\n",
    "\n",
    "        params_opt = cv_overview.iloc[\n",
    "            cv_overview[\"mean_mse\"].argmin(),\n",
    "        ].name\n",
    "\n",
    "    else:\n",
    "        trained_on_fold_1[\"mse_1\"] = np.nan\n",
    "        prod = product(trained_on_fold_1.index.get_level_values('gamma').unique(),trained_on_fold_1.index.get_level_values('lamda').unique())\n",
    "        for gamma, lamda in prod:\n",
    "            index = (gamma, lamda)\n",
    "            y_hat_1, mse_1 = make_prediction(X= X_fold_2, \n",
    "                            y=y_fold_2,\n",
    "                            theta_nat= trained_on_fold_1.at[index, \"theta_nat\"],\n",
    "                            intercept = intercept)\n",
    "\n",
    "            trained_on_fold_1.at[index, \"mse_1\"] = mse_1\n",
    "\n",
    "        cv_overview = trained_on_fold_1[['mse_1']]\n",
    "\n",
    "        params_opt = cv_overview.iloc[cv_overview['mse_1'].argmin(),].name\n",
    "\n",
    "    return cv_overview, params_opt\n",
    "\n",
    "\n",
    "def adaptive_lasso_tuned(X, y, first_stage=\"OLS\", intercept=True, cross_valid_split = True):\n",
    "\n",
    "    \"\"\"Tuned (i.e. cross-validated) version of the adaptive lasso.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): regressor matrix of shape (n, p)\n",
    "        y (np.ndarray): vector of the dependent variable *y*, of shape (n, 1)\n",
    "        first_stage (str): Options are \"OLS\" and \"Lasso\" currently. Determines which method should be used for getting initial first-stage estimates for the coefficient vector. Defaults to \"OLS\". If \"Lasso\" is chosen, the full *lamda_path* is calculated and the lamda that minimzes BIC is taken as a final estimate (for selection consistency), following **Zou, H., Hastie, T., & Tibshirani, R. (2007)**\n",
    "        intercept (bool): logical value whether an intercept shall be used while fitting the adaptive lasso for *theta_nat*\n",
    "        cross_valid_split (bool): Option to turn-off the exchange of the two cross-validation folds, thus evaluation is only done once on the second fold, while training is done on the first fold. In small samples this is currrently necessary, due to issues with float multiindexing.\n",
    "\n",
    "    Returns:\n",
    "        dict: dict containing:\n",
    "\n",
    "            **selected_support** (*np.ndarray*): logical vector of the coefficients that are active in the optimal adaptive lasso fit \\n\n",
    "            **theta_opt_nat** (*np.ndarray*): optimal coefficient vector from fitting and cross-validating the adaptive lasso, on original scale \\n\n",
    "            **theta_opt_std** (*np.ndarray*): optimal coefficient vector from fitting and cross-validating the adaptive lasso, on standardized scale \\n\n",
    "            **conf_intervals_nat** (*np.ndarray*): elementwise confidence intervals at the 95% confidence level for optimal coefficients in the active set, on original scale \\n\n",
    "            **conf_intervals_std** (*np.ndarray*): elementwise confidence intervals at the 95% confidence level for optimal coefficients in the active set, on standardized scale\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X_standardized = (X - x_mean) / x_std\n",
    "    y_standardized = (y - y_mean) / y_std\n",
    "\n",
    "    cv_results, params_opt = cv_adaptive_lasso(\n",
    "        X=X, y=y, intercept=intercept, first_stage=first_stage, cross_valid_split= cross_valid_split\n",
    "    )\n",
    "    gamma_opt = params_opt[0]\n",
    "    lamda_opt = params_opt[1]\n",
    "\n",
    "    train_opt_ada_lasso = adaptive_lasso(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        intercept=intercept,\n",
    "        lamda_path=np.array([lamda_opt]),\n",
    "        gamma_path=np.array([gamma_opt]),\n",
    "        first_stage=first_stage,\n",
    "        num_iters=100,\n",
    "        out_as_df=True,\n",
    "    )\n",
    "\n",
    "    ada_lasso_opt_res = get_conf_intervals(\n",
    "        lamda=lamda_opt,\n",
    "        weights=train_opt_ada_lasso.iloc[0][\"gamma_weights\"],\n",
    "        theta_std=train_opt_ada_lasso.iloc[0][\"theta_std\"],\n",
    "        theta_nat=train_opt_ada_lasso.iloc[0][\"theta_nat\"],\n",
    "        X=X,\n",
    "        X_std=X_standardized,\n",
    "        intercept=intercept,\n",
    "        y=y,\n",
    "        y_std=y_standardized,\n",
    "    )\n",
    "\n",
    "    selected_support = ada_lasso_opt_res[\"active_set\"]\n",
    "    conf_intervals_nat = ada_lasso_opt_res[\"conf_intervals_nat\"]\n",
    "    conf_intervals_std = ada_lasso_opt_res[\"conf_intervals_std\"]\n",
    "\n",
    "    return {\n",
    "        \"selected_support\": selected_support,\n",
    "        \"theta_opt_nat\": train_opt_ada_lasso.iloc[0][\"theta_nat\"],\n",
    "        \"theta_opt_std\": train_opt_ada_lasso.iloc[0][\"theta_std\"],\n",
    "        \"conf_intervals_nat\": conf_intervals_nat,\n",
    "        \"conf_intervals_std\": conf_intervals_std,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
