{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alien-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from numba import njit, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "paperback-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(rho, lamda, w):\n",
    "    '''Soft threshold function used for normalized data and lasso regression'''\n",
    "    if rho < - lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho >  lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minus-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lasso(theta, X , y, penalty_factors, lamda = 0.001, num_iters = 1000, intercept = False):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "    \n",
    "    if intercept == False:\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        # we drop the constant from the regression, since all variables, including the dependent variable, are standardized\n",
    "        X = np.delete(X, 0, 1)\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    for i in range(num_iters): \n",
    "        for j in range(p):\n",
    "            w_j = penalty_factors[j]\n",
    "            X_j = X[:,j].reshape(-1,1)\n",
    "            y_pred = X @ theta\n",
    "            rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "            z = np.sum(np.square(X_j))\n",
    "        \n",
    "            #if intercept == True:  \n",
    "            #    if j == 0: \n",
    "            #        theta[j] =  rho / z\n",
    "            #    else:\n",
    "            #        theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)  \n",
    "\n",
    "            theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)   \n",
    "            \n",
    "    return theta.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "threaded-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "X = np.random.rand(50,3)\n",
    "m, p = X.shape\n",
    "#X = np.insert(X, 0, 1, axis=1)\n",
    "y = np.array(1.5 * X[:,0] - 14.5 * X[:,1] + 5, dtype=np.float64).reshape(-1,1)\n",
    "initial_theta = np.ones((p,1))\n",
    "pen = np.ones((p,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "royal-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.insert(X, 0, np.array(1.5 * X[:,0] - 14.5 * X[:,1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cross-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"test_lasso.csv\", data , delimiter=',', header=\"y,X_0,X_1\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dynamic-dialogue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "another-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.3262"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.9 * 0.4365 - 9.5 * 0.8817 + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "solid-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = adaptive_lasso(theta=initial_theta, X=X, y=y, penalty_factors = pen, intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "related-manitoba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72464173, -0.72317067])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "perceived-kingston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.99131208, -2.98523955])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.delete(X, 0, 1)\n",
    "(betas / np.std(X)) * np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "generous-template",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000000047248456"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas[0] - ( np.sum(  (X[:,1:].mean(axis=0) / np.std(X[:,1:], axis=0)) * betas[1:] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "charged-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(1000,2)\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "y = np.array(3 * X[:,1] - 3 * X[:,2] + 20* X[:,0]).reshape(-1,1)\n",
    "m, p = X.shape\n",
    "initial_theta = np.ones((p,1))\n",
    "pen = np.ones((p,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "superior-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_2 = adaptive_lasso_intercept(theta=initial_theta, X=X, y=y, penalty_factors = pen, intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "popular-invalid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.020455  ,  0.88032817, -0.877247  ])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "together-funds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.36443892,  1.07133939, -1.06758968])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(betas_2 / np.std(X)) * np.std(y)\n",
    "(betas_2) * np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "immune-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lasso_intercept(theta, X , y, penalty_factors, lamda = 0.001, num_iters = 1000, intercept = True):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "    \n",
    "    if intercept == False:\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        # add intercept column\n",
    "        X = scaler.fit_transform(X)\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    for i in range(num_iters): \n",
    "        for j in range(p):\n",
    "            w_j = penalty_factors[j]\n",
    "            X_j = X[:,j].reshape(-1,1)\n",
    "            y_pred = X @ theta\n",
    "            rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "            z = np.sum(np.square(X_j))\n",
    "        \n",
    "            if intercept == True:  \n",
    "                if j == 0: \n",
    "                    theta[j] =  rho / z\n",
    "                else:\n",
    "                    theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j) \n",
    "                    \n",
    "            else:\n",
    "                theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)   \n",
    "            \n",
    "    return theta.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "agricultural-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, p = X.shape\n",
    "intercept = True\n",
    "if intercept:\n",
    "    p = p + 1\n",
    "\n",
    "initial_theta = np.ones((p,1))\n",
    "pen = np.ones((p,1))\n",
    "\n",
    "betas = adaptive_lasso_intercept(theta=initial_theta, X = X , y = y, penalty_factors = pen, intercept = intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "commercial-longitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75823023e-14,  1.02579599e-01, -9.94392115e-01])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "liberal-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.56740532e-13,  1.49788920e+00, -1.45203260e+01])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas / np.std(X) * np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "local-breakfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.56740532e-13,  1.49788920e+00, -1.45203260e+01])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas / np.std(X) * np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "extreme-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0005240897670795"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(betas[0] - (np.sum(  (X[:,1:].mean(axis=0) / np.std(X[:,1:], axis=0)) * betas[1:]))) * np.std(y) + np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "completed-apollo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = Lasso(alpha=0.001, fit_intercept = False)\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "y_std = scaler.fit_transform(y)\n",
    "y_std = y_std.flatten()\n",
    "# Fit the lasso regression\n",
    "reg.fit(X_std, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "silver-enterprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10158284 -0.99339536]\n"
     ]
    }
   ],
   "source": [
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "molecular-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "passing-division",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49912737, 0.50018097])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,:].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dimensional-still",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28887777651043417"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "tracked-jacket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.62278928, 29.16354553])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "x_mean = X[:,:].mean(axis=0)\n",
    "x_std = X[:,:].std(axis=0)\n",
    "y_std = np.std(y)\n",
    "\n",
    "a / x_std * y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "flush-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = X[:,:].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "adopted-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2884713 , 0.28928273])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "portuguese-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50018097])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "measured-battlefield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50018097, 0.49912737, 0.50018097])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((x_mean[1:], x_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "important-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lasso_intercept(theta, X , y, penalty_factors, lamda = 0.001, num_iters = 1000, intercept = True):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "\n",
    "    x_mean = X[:,:].mean(axis=0)\n",
    "    x_std = X[:,:].std(axis=0)\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "    \n",
    "    if intercept == False:\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        # add intercept column\n",
    "        X = scaler.fit_transform(X)\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    for i in range(num_iters): \n",
    "        for j in range(p):\n",
    "            w_j = penalty_factors[j]\n",
    "            X_j = X[:,j].reshape(-1,1)\n",
    "            y_pred = X @ theta\n",
    "            rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "            z = np.sum(np.square(X_j))\n",
    "        \n",
    "            if intercept == True:  \n",
    "                if j == 0: \n",
    "                    theta[j] =  rho / z\n",
    "                else:\n",
    "                    theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j) \n",
    "                    \n",
    "            else:\n",
    "                theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)   \n",
    "\n",
    "    if intercept == False:\n",
    "        theta_nat = theta.flatten() / x_std * y_std\n",
    "    if intercept == True:\n",
    "        theta_0 = (theta.flatten()[0] - np.sum((x_mean[1:] / x_std[1:]) * theta.flatten()[1:])) * y_std + y_mean\n",
    "        theta_betas = theta.flatten()[1:] / x_std[1:] * y_std\n",
    "        theta_nat = np.insert(arr = theta_betas, obj = 0, values= theta_0)\n",
    "        \n",
    "    return [theta.flatten(), theta_nat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "speaking-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, p = X.shape\n",
    "intercept = True\n",
    "if intercept:\n",
    "    p = p + 1\n",
    "\n",
    "initial_theta = np.ones((p,1))\n",
    "pen = np.ones((p,1))\n",
    "\n",
    "betas, betas_nat = adaptive_lasso_intercept(theta=initial_theta, X = X , y = y, penalty_factors = pen, intercept = intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "furnished-giving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75823023e-14,  1.02579599e-01, -9.94392115e-01])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "opposed-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(X, y, theta, active_set, penalty_factors, intercept, lamda, thresh, active_thresh):\n",
    "    active_set_converged_check = np.full((len(active_set), ), False)\n",
    "    active_set_update = np.full((len(active_set), ), True)\n",
    "    \n",
    "    #if determine_null_dev:\n",
    "    #    null_dev = np.ones((len(active_set), ))\n",
    "    \n",
    "    for subindex, j in enumerate(active_set):\n",
    "        w_j = penalty_factors[j]\n",
    "        X_j = X[:,j].reshape(-1,1)\n",
    "\n",
    "        y_pred = X @ theta\n",
    "        rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "        z = np.sum(np.square(X_j))\n",
    "\n",
    "        if intercept == True:  \n",
    "            if j == 0:\n",
    "                tmp = rho / z\n",
    "                #if determine_null_dev:\n",
    "                #    null_dev[j] = np.abs(theta[j] - tmp)\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] =  tmp\n",
    "            else:\n",
    "                tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "                #if determine_null_dev:\n",
    "                #    null_dev[j] = np.abs(theta[j] - tmp)\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] =  tmp\n",
    "\n",
    "        else:\n",
    "            tmp = (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            #if determine_null_dev:\n",
    "            #    null_dev[j] = np.abs(theta[j] - tmp)\n",
    "            if np.abs(tmp) < active_thresh:\n",
    "                active_set_update[subindex] = False\n",
    "            if np.abs(theta[j] - tmp) < thresh:\n",
    "                active_set_converged_check[subindex] = True\n",
    "            theta[j] =  tmp\n",
    "    \n",
    "    active_set_converged = np.all(active_set_converged_check)\n",
    "    active_set = active_set[active_set_update]\n",
    "    \n",
    "    #if determine_null_dev:\n",
    "    #    return  [theta, active_set, active_set_converged, null_dev]\n",
    "    #else:\n",
    "    return  [theta, active_set, active_set_converged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "blank-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lamda_path(X, y, epsilon = 0.0001, K = 100):\n",
    "    # Calculate lambda path\n",
    "    # get lambda_max\n",
    "    m, p = X.shape\n",
    "\n",
    "    y = y.reshape((m,1))\n",
    "    sx = X\n",
    "    sy = y\n",
    "\n",
    "    lambda_max = np.max(np.abs(np.sum(sx*sy, axis=0))) / m\n",
    "    lamda_path = np.exp(np.linspace(start= np.log(lambda_max), stop=np.log(lambda_max*epsilon),num= np.int64(K)))\n",
    "\n",
    "    return lamda_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "physical-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lasso_intercept(X , y, penalty_factors = None, theta = None, lamda_path = \"auto\", num_iters = 100, intercept = True, thresh = 1e-7, warm_start = True):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "    \n",
    "    #x_mean = X[:,:].mean(axis=0)\n",
    "    #x_std = X[:,:].std(axis=0)\n",
    "    \n",
    "    #m, p = X.shape\n",
    "    #x_mean = np.zeros((p,),dtype=np.float64)\n",
    "    #for i in range(p):\n",
    "    #    x_mean[i] = X[:,i].mean()\n",
    "        \n",
    "    #x_std = np.zeros((p,),dtype=np.float64)\n",
    "    #for i in range(p):\n",
    "    #    x_std[i] = X[:,i].std()\n",
    "    \n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "    X = scaler.fit_transform(X)\n",
    "    #X_std = (X - x_mean) / x_std\n",
    "    #y = (y - y_mean) / y_std\n",
    "    \n",
    "    if lamda_path == \"auto\":\n",
    "        path = get_lamda_path(X = X, y = y, epsilon = 0.0001, K = 100)\n",
    "    else:\n",
    "        path = [0.001]\n",
    "        \n",
    "    if intercept == True:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        #X = np.ones((m, p + 1))\n",
    "        #X[:,1:] = X_std\n",
    "    \n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        output = {}\n",
    "        output['lamda'] = lamda\n",
    "        tol_vals = np.full((p, ), False)\n",
    "        \n",
    "        #print(\"next lamda\")\n",
    "        for i in range(num_iters):\n",
    "            #print(f\"Theta before loop {i} : {theta}\")\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    #print(\"start_theta:\")\n",
    "                    #print(theta)\n",
    "                    w_j = penalty_factors[j]\n",
    "                    X_j = X[:,j].reshape(-1,1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z ) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  rho / z\n",
    "                        else:\n",
    "                            if np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j) \n",
    "\n",
    "                    else:\n",
    "                        if np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                        theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "            #print(f\"Theta after loop {i} : {theta}\")\n",
    "                \n",
    "            \n",
    "        if intercept == False:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr = theta_betas, obj = 0, values= theta_0)\n",
    "\n",
    "        output['theta_std'] = theta.flatten()\n",
    "        output['theta_nat'] = theta_nat\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "oriental-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_lasso():\n",
    "    path = get_lamda_path(X = X, y = y, epsilon = 0.0001, K = 100)\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    y_std = scaler.fit_transform(y)\n",
    "    y_std = y_std.flatten()\n",
    "\n",
    "    for i in path:\n",
    "        reg = Lasso(alpha= i, fit_intercept = True)\n",
    "        reg.fit(X_std, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dirty-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lasso(X , y, penalty_factors = None, theta = None, lamda_path = \"auto\", num_iters = 100, intercept = True, thresh = 1e-7, active_thresh = 1e-7):\n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X = (X - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "\n",
    "    if lamda_path == \"auto\":\n",
    "        path = get_lamda_path(X = X, y = y, epsilon = 0.0001, K = 100)\n",
    "    else:\n",
    "        path = lamda_path\n",
    "    \n",
    "    if intercept == True:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        output = {}\n",
    "        output['lamda'] = lamda\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "        \n",
    "        for i in range(num_iters): \n",
    "            #print(str(i))\n",
    "             #if i == 0:\n",
    "             #   theta, active_set, active_set_converged, null_dev = update_coeffs(X = X, \n",
    "             #                                                           y = y,\n",
    "             #                                                           theta = theta, \n",
    "             #                                                           active_set = active_set, \n",
    "             #                                                           penalty_factors = penalty_factors, \n",
    "             #                                                           intercept = intercept, \n",
    "             #                                                           lamda = lamda, \n",
    "             #                                                           thresh = thresh, \n",
    "             #                                                           active_thresh = active_thresh,\n",
    "             #                                                           determine_null_dev = True)\n",
    "                #thresh_inner = thresh_inner * null_dev\n",
    "                #print(\"Done\")\n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                theta, active_set, active_set_converged = update_coeffs(X = X, \n",
    "                                                                        y = y,\n",
    "                                                                        theta = theta, \n",
    "                                                                        active_set = active_set, \n",
    "                                                                        penalty_factors = penalty_factors, \n",
    "                                                                        intercept = intercept, \n",
    "                                                                        lamda = lamda, \n",
    "                                                                        thresh = thresh, \n",
    "                                                                        active_thresh = active_thresh)\n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "                theta, active_set, active_set_converged = update_coeffs(X = X,\n",
    "                                                                        y = y,\n",
    "                                                                        theta = theta, \n",
    "                                                                        active_set = active_set, \n",
    "                                                                        penalty_factors = penalty_factors, \n",
    "                                                                        intercept = intercept, \n",
    "                                                                        lamda = lamda, \n",
    "                                                                        thresh = thresh, \n",
    "                                                                        active_thresh = active_thresh)\n",
    "                #print(f\"Theta before loop {i} : {theta}\")\n",
    "                #print(active_set)\n",
    "                #print(\"Active_set_converged:\")\n",
    "                #print(active_set_converged)\n",
    "                \n",
    "                if active_set_converged:\n",
    "                    #print(\"Sec. check successful!\")\n",
    "                    sec_check_all_converged = True\n",
    "                    #print(f\"Final theta : {theta}\")\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        if intercept == False:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr = theta_betas, obj = 0, values= theta_0)\n",
    "\n",
    "        output['theta_std'] = theta.flatten()\n",
    "        output['theta_nat'] = theta_nat\n",
    "        result.append(output)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "electrical-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "X = np.random.rand(4000,300)\n",
    "y = np.array(1.5 * X[:,0] - 14.5 * X[:,1] + 5, dtype=np.float64).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "immediate-importance",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-1d9f0b846015>\u001b[0m in \u001b[0;36madaptive_lasso_intercept\u001b[0;34m(X, y, penalty_factors, theta, lamda_path, num_iters, intercept, thresh, warm_start)\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mX_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_old = adaptive_lasso_intercept(X = X , y =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "foster-arrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 243 ms, total: 4.84 s\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = adaptive_lasso(X = X, y = y, lamda_path = \"auto\", thresh = 1e-7, active_thresh = 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "false-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.64 s, sys: 370 ms, total: 8.01 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fantastic-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 1.86 s, total: 2min 43s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = adaptive_lasso_numba(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "accomplished-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 1.53 s, total: 2min 21s\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = adaptive_lasso_numba_intercept(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "continent-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prescription-province",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evenly sampled time at 200ms intervals\n",
    "t = [200,100,500, 1000]\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, [0.890,0.447, 4.06, 30.7] , 'r--', t, [0.286,0.206, 0.586, 1.01], 'b--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "strange-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def soft_threshold_numba(rho, lamda, w):\n",
    "    if rho < - lamda * w:\n",
    "        return rho + lamda * w\n",
    "    elif rho >  lamda * w:\n",
    "        return rho - lamda * w\n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "minor-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def adaptive_lasso_numba_intercept(X , y, penalty_factors = None, theta = None, lamda_path = None, num_iters = 100, intercept = True, thresh = 1e-7):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "    \n",
    "    m, p = X.shape\n",
    "    x_mean = np.zeros((p,),dtype=np.float64)\n",
    "    for i in range(p):\n",
    "        x_mean[i] = X[:,i].mean()\n",
    "        \n",
    "    x_std = np.zeros((p,),dtype=np.float64)\n",
    "    for i in range(p):\n",
    "        x_std[i] = X[:,i].std()\n",
    "    \n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X = (X - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "    \n",
    "    #if lamda_path == \"auto\":\n",
    "    #    path = [0.001]\n",
    "    path = [2.857874270239155,\n",
    " 2.603988596702651,\n",
    " 2.3726574266648925,\n",
    " 2.1618770801978675,\n",
    " 1.9698218787759958,\n",
    " 1.7948283321221274,\n",
    " 1.6353807298505643,\n",
    " 1.4900980131088004,\n",
    " 1.357721812506429,\n",
    " 1.2371055487214755,\n",
    " 1.1272045013788243,\n",
    " 1.0270667601821153,\n",
    " 0.9358249799221423,\n",
    " 0.8526888679475817,\n",
    " 0.7769383390281148,\n",
    " 0.7079172783206475,\n",
    " 0.6450278584164171,\n",
    " 0.5877253612459741,\n",
    " 0.5355134599918534,\n",
    " 0.4879399201431188,\n",
    " 0.44459268245637595,\n",
    " 0.40509629389573,\n",
    " 0.3691086556382036,\n",
    " 0.33631805997738856,\n",
    " 0.3064404904604118,\n",
    " 0.27921716187329104,\n",
    " 0.2544122787672127,\n",
    " 0.23181099311115563,\n",
    " 0.21121754338102922,\n",
    " 0.19245355896700148,\n",
    " 0.17535651521260795,\n",
    " 0.15977832570392755,\n",
    " 0.14558405961590895,\n",
    " 0.13265077300611328,\n",
    " 0.1208664439331003,\n",
    " 0.11012900217596196,\n",
    " 0.1003454451508982,\n",
    " 0.09143103236732805,\n",
    " 0.08330855144629912,\n",
    " 0.07590764934379896,\n",
    " 0.06916422298634396,\n",
    " 0.06301986404082344,\n",
    " 0.05742135300946009,\n",
    " 0.05232019826798009,\n",
    " 0.0476722160543616,\n",
    " 0.043437147770225164,\n",
    " 0.03957831128011415,\n",
    " 0.03606228318838558,\n",
    " 0.03285860934174666,\n",
    " 0.02993954104994806,\n",
    " 0.027279794739903485,\n",
    " 0.024856332961475294,\n",
    " 0.022648164848102118,\n",
    " 0.020636164303954655,\n",
    " 0.018802904342843405,\n",
    " 0.017132506144001115,\n",
    " 0.0156105015173336,\n",
    " 0.014223707586878702,\n",
    " 0.012960112607041186,\n",
    " 0.011808771922598759,\n",
    " 0.010759713171334284,\n",
    " 0.009803849908205072,\n",
    " 0.008932902902902731,\n",
    " 0.008139328429120903,\n",
    " 0.007416252924406953,\n",
    " 0.006757413454652204,\n",
    " 0.006157103467552789,\n",
    " 0.0056101233651836314,\n",
    " 0.0051117354675686184,\n",
    " 0.0046576229771631275,\n",
    " 0.004243852588818783,\n",
    " 0.0038668404213759743,\n",
    " 0.003523320975799717,\n",
    " 0.0032103188509891834,\n",
    " 0.0029251229722768134,\n",
    " 0.002665263109396476,\n",
    " 0.002428488480530631,\n",
    " 0.0022127482571150047,\n",
    " 0.0020161738005426556,\n",
    " 0.0018370624769102856,\n",
    " 0.0016738629096179224,\n",
    " 0.0015251615420869573,\n",
    " 0.0013896703942093034,\n",
    " 0.0012662159064798467,\n",
    " 0.001153728775185449,\n",
    " 0.0010512346906077228,\n",
    " 0.0009578458980183457,\n",
    " 0.0008727535083723128,\n",
    " 0.0007952204920979788,\n",
    " 0.0007245752952995095,\n",
    " 0.0006602060230782952,\n",
    " 0.0006015551395920666,\n",
    " 0.000548114638946145,\n",
    " 0.0004994216450894154,\n",
    " 0.000455054402603404,\n",
    " 0.0004146286236586048,\n",
    " 0.0003777941594971461,\n",
    " 0.0003442319676117548,\n",
    " 0.00031365134835218414,\n",
    " 0.0002857874270239154]\n",
    "        \n",
    "    if intercept == True:\n",
    "        X_tmp = np.ones((m, p + 1))\n",
    "        X_tmp[:,1:] = X\n",
    "        X = X_tmp\n",
    "    \n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    lamdas = []\n",
    "    thetas = []\n",
    "    thetas_nat = []\n",
    "    for lamda in path:\n",
    "        tol_vals = np.full((p, ), False)\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "            #print(f\"Theta before loop {i} : {theta}\")\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    #print(\"start_theta:\")\n",
    "                    #print(theta)\n",
    "                    w_j = penalty_factors[j].item()\n",
    "                    #X_j = X[:,[j]]\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "                    for obs in range(m):\n",
    "                        rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                        z += np.square(X[obs,j].item())\n",
    "\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z ) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  rho / z\n",
    "                        else:\n",
    "                            if np.abs(theta[j] - (1 / z) * soft_threshold_numba(rho, lamda, w_j)) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  (1 / z) * soft_threshold_numba(rho, lamda, w_j) \n",
    "\n",
    "                    else:\n",
    "                        if np.abs(theta[j] - (1 / z) * soft_threshold_numba(rho, lamda, w_j)) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                        theta[j] =  (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "            #print(f\"Theta after loop {i} : {theta}\")\n",
    "                \n",
    "            \n",
    "        if intercept == False:\n",
    "            theta_tmp = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_tmp = np.ones((p,))\n",
    "            theta_tmp[1:] = theta_betas\n",
    "            theta_tmp[0] = theta_0\n",
    "\n",
    "        #output[1] = theta\n",
    "        #output[2] = theta_nat\n",
    "        lamdas.append(lamda)\n",
    "        thetas.append(theta)\n",
    "        thetas_nat.append(theta_tmp)\n",
    "  \n",
    "    \n",
    "    return lamdas,thetas,thetas_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def update_coeffs_numba(X, y, theta, active_set, penalty_factors, intercept, lamda, thresh, active_thresh):\n",
    "    active_set_converged_check = np.full((len(active_set), ), False)\n",
    "    active_set_update = np.full((len(active_set), ), True)\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    rho = []\n",
    "    z = []\n",
    "    for j in range(p):\n",
    "        rho_j = 0.0\n",
    "        z_j = 0.0\n",
    "        for obs in range(m):\n",
    "                rho_j += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                z_j += np.square(X[obs,j].item())\n",
    "                \n",
    "        rho.append(rho_j)\n",
    "        z.append(z_j)\n",
    "                \n",
    "            \n",
    "    for subindex, j in enumerate(active_set):\n",
    "        w_j = penalty_factors[j].item()\n",
    "        \n",
    "        y_pred = X @ theta\n",
    "        #rho = 0.0\n",
    "        #z = 0.0\n",
    "        \n",
    "        #for obs in range(m):\n",
    "        #    rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "        #    z += np.square(X[obs,j].item())\n",
    "\n",
    "        if intercept == True:  \n",
    "            if j == 0:\n",
    "                tmp = rho / z\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] =  tmp\n",
    "            else:\n",
    "                tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                if np.abs(tmp) < active_thresh:\n",
    "                    active_set_update[subindex] = False\n",
    "                if np.abs(theta[j] - tmp) < thresh:\n",
    "                    active_set_converged_check[subindex] = True\n",
    "                theta[j] =  tmp\n",
    "\n",
    "        else:\n",
    "            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "            if np.abs(tmp) < active_thresh:\n",
    "                active_set_update[subindex] = False\n",
    "            if np.abs(theta[j] - tmp) < thresh:\n",
    "                active_set_converged_check[subindex] = True\n",
    "            theta[j] =  tmp\n",
    "    \n",
    "    active_set_converged = np.all(active_set_converged_check)\n",
    "    active_set = active_set[active_set_update]\n",
    "    \n",
    "    return  [theta, active_set, active_set_converged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "protecting-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def adaptive_lasso_numba(X , y, penalty_factors = None, theta = None, lamda_path = \"auto\", num_iters = 100, intercept = True, thresh = 1e-2, active_thresh = 1e-2):\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    x_mean = np.zeros((p,),dtype=np.float64)\n",
    "    \n",
    "    for i in range(p):\n",
    "        x_mean[i] = X[:,i].mean()\n",
    "        \n",
    "    x_std = np.zeros((p,),dtype=np.float64)\n",
    "    \n",
    "    for i in range(p):\n",
    "        x_std[i] = X[:,i].std()\n",
    "    \n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X = (X - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "    \n",
    "\n",
    "    path = [2.857874270239155,\n",
    "             2.603988596702651,\n",
    "             2.3726574266648925,\n",
    "             2.1618770801978675,\n",
    "             1.9698218787759958,\n",
    "             1.7948283321221274,\n",
    "             1.6353807298505643,\n",
    "             1.4900980131088004,\n",
    "             1.357721812506429,\n",
    "             1.2371055487214755,\n",
    "             1.1272045013788243,\n",
    "             1.0270667601821153,\n",
    "             0.9358249799221423,\n",
    "             0.8526888679475817,\n",
    "             0.7769383390281148,\n",
    "             0.7079172783206475,\n",
    "             0.6450278584164171,\n",
    "             0.5877253612459741,\n",
    "             0.5355134599918534,\n",
    "             0.4879399201431188,\n",
    "             0.44459268245637595,\n",
    "             0.40509629389573,\n",
    "             0.3691086556382036,\n",
    "             0.33631805997738856,\n",
    "             0.3064404904604118,\n",
    "             0.27921716187329104,\n",
    "             0.2544122787672127,\n",
    "             0.23181099311115563,\n",
    "             0.21121754338102922,\n",
    "             0.19245355896700148,\n",
    "             0.17535651521260795,\n",
    "             0.15977832570392755,\n",
    "             0.14558405961590895,\n",
    "             0.13265077300611328,\n",
    "             0.1208664439331003,\n",
    "             0.11012900217596196,\n",
    "             0.1003454451508982,\n",
    "             0.09143103236732805,\n",
    "             0.08330855144629912,\n",
    "             0.07590764934379896,\n",
    "             0.06916422298634396,\n",
    "             0.06301986404082344,\n",
    "             0.05742135300946009,\n",
    "             0.05232019826798009,\n",
    "             0.0476722160543616,\n",
    "             0.043437147770225164,\n",
    "             0.03957831128011415,\n",
    "             0.03606228318838558,\n",
    "             0.03285860934174666,\n",
    "             0.02993954104994806,\n",
    "             0.027279794739903485,\n",
    "             0.024856332961475294,\n",
    "             0.022648164848102118,\n",
    "             0.020636164303954655,\n",
    "             0.018802904342843405,\n",
    "             0.017132506144001115,\n",
    "             0.0156105015173336,\n",
    "             0.014223707586878702,\n",
    "             0.012960112607041186,\n",
    "             0.011808771922598759,\n",
    "             0.010759713171334284,\n",
    "             0.009803849908205072,\n",
    "             0.008932902902902731,\n",
    "             0.008139328429120903,\n",
    "             0.007416252924406953,\n",
    "             0.006757413454652204,\n",
    "             0.006157103467552789,\n",
    "             0.0056101233651836314,\n",
    "             0.0051117354675686184,\n",
    "             0.0046576229771631275,\n",
    "             0.004243852588818783,\n",
    "             0.0038668404213759743,\n",
    "             0.003523320975799717,\n",
    "             0.0032103188509891834,\n",
    "             0.0029251229722768134,\n",
    "             0.002665263109396476,\n",
    "             0.002428488480530631,\n",
    "             0.0022127482571150047,\n",
    "             0.0020161738005426556,\n",
    "             0.0018370624769102856,\n",
    "             0.0016738629096179224,\n",
    "             0.0015251615420869573,\n",
    "             0.0013896703942093034,\n",
    "             0.0012662159064798467,\n",
    "             0.001153728775185449,\n",
    "             0.0010512346906077228,\n",
    "             0.0009578458980183457,\n",
    "             0.0008727535083723128,\n",
    "             0.0007952204920979788,\n",
    "             0.0007245752952995095,\n",
    "             0.0006602060230782952,\n",
    "             0.0006015551395920666,\n",
    "             0.000548114638946145,\n",
    "             0.0004994216450894154,\n",
    "             0.000455054402603404,\n",
    "             0.0004146286236586048,\n",
    "             0.0003777941594971461,\n",
    "             0.0003442319676117548,\n",
    "             0.00031365134835218414,\n",
    "             0.0002857874270239154]\n",
    "    \n",
    "    if intercept == True:\n",
    "        X_tmp = np.ones((m, p + 1))\n",
    "        X_tmp[:,1:] = X\n",
    "        X = X_tmp\n",
    "    \n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    lamdas = []\n",
    "    thetas = []\n",
    "    thetas_nat = []\n",
    "    \n",
    "    for lamda in path:\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "        \n",
    "        for i in range(num_iters): \n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                active_set_converged_check = np.full((len(active_set), ), False)\n",
    "                active_set_update = np.full((len(active_set), ), True)\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    \n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                        z += np.square(X[obs,j].item())\n",
    "                \n",
    "                    ###\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] =  tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "                \n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "                \n",
    "                active_set_converged_check = np.full((len(active_set), ), False)\n",
    "                active_set_update = np.full((len(active_set), ), True)\n",
    "\n",
    "                m, p = X.shape\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                        z += np.square(X[obs,j].item())\n",
    "\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] =  tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "                \n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if intercept == False:\n",
    "            theta_tmp = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_tmp = np.ones((p,))\n",
    "            theta_tmp[1:] = theta_betas\n",
    "            theta_tmp[0] = theta_0\n",
    "        \n",
    "        \n",
    "        lamdas.append(lamda)\n",
    "        thetas.append(theta)\n",
    "        thetas_nat.append(theta_tmp)\n",
    "\n",
    "    return lamdas,thetas,thetas_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "western-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_thresh_adaptive_lasso(X , y, penalty_factors = None, theta = None, lamda_path = \"auto\", num_iters = 100, intercept = True, thresh = 1e-7):\n",
    "    '''Coordinate gradient descent for lasso regression - for standardized data ''' \n",
    "    x_mean = X.mean(axis=0)\n",
    "    x_std = X.std(axis=0)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X = (X - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "    \n",
    "    if lamda_path == \"auto\":\n",
    "        path = get_lamda_path(X = X, y = y, epsilon = 0.0001, K = 100)\n",
    "    else:\n",
    "        path = lamda_path\n",
    "        \n",
    "    if intercept == True:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    result = []\n",
    "    for lamda in path:\n",
    "        theta = np.zeros((p,1))\n",
    "        output = {}\n",
    "        output['lamda'] = lamda\n",
    "        tol_vals = np.full((p, ), False)\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "            if not np.all(tol_vals):\n",
    "                for j in range(p):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "                    X_j = X[:,j].reshape(-1,1)\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = X_j.T @ (y - y_pred  + theta[j]*X_j)\n",
    "                    z = np.sum(np.square(X_j))\n",
    "\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            if np.abs(theta[j] - rho / z ) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  rho / z\n",
    "                        else:\n",
    "                            if np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)) < thresh:\n",
    "                                tol_vals[j] = True\n",
    "                            theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j) \n",
    "\n",
    "                    else:\n",
    "                        if np.abs(theta[j] - (1 / z) * soft_threshold(rho, lamda, w_j)) < thresh:\n",
    "                            tol_vals[j] = True\n",
    "                        theta[j] =  (1 / z) * soft_threshold(rho, lamda, w_j)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if intercept == False:\n",
    "            theta_nat = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_nat = np.insert(arr = theta_betas, obj = 0, values= theta_0)\n",
    "\n",
    "        output['theta_std'] = theta.flatten()\n",
    "        output['theta_nat'] = theta_nat\n",
    "        result.append(output)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "administrative-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "X = np.random.rand(300,150)\n",
    "y = np.array(1.5 * X[:,0] - 14.5 * X[:,1] + 5, dtype=np.float64).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "digital-envelope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 3.22 s, total: 3min 32s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = naive_adaptive_lasso(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "going-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 2.04 s, total: 2min 13s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res2 = eps_thresh_adaptive_lasso(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "royal-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 1.24 s, total: 1min 20s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res3 = eps_thresh_adaptive_lasso_warm_start(X = X, y = y, lamda_path = \"auto\", warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "third-allocation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 207 ms, sys: 30.2 ms, total: 237 ms\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "casual-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 53.3 ms, total: 1.07 s\n",
      "Wall time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = lasso_numba(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cutting-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 237 ms, total: 10.5 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res4 = adaptive_lasso(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "public-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def lasso_numba(X , y, penalty_factors = None, theta = None, lamda_path = \"auto\", lamda_path_custom = np.array([0.01]), num_iters = 100, intercept = True, thresh = 1e-7, active_thresh = 1e-7, warm_start = True):\n",
    "    \n",
    "    m, p = X.shape\n",
    "    \n",
    "    x_mean = np.zeros((p,),dtype=np.float64)\n",
    "    \n",
    "    for i in range(p):\n",
    "        x_mean[i] = X[:,i].mean()\n",
    "        \n",
    "    x_std = np.zeros((p,),dtype=np.float64)\n",
    "    \n",
    "    for i in range(p):\n",
    "        x_std[i] = X[:,i].std()\n",
    "    \n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "\n",
    "    X = (X - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "    \n",
    "    if lamda_path == \"auto\":\n",
    "        path = get_lamda_path_numba(X = X, y = y)\n",
    "    else:\n",
    "        path = lamda_path_custom\n",
    "    \n",
    "    \n",
    "    if intercept == True:\n",
    "        X_tmp = np.ones((m, p + 1))\n",
    "        X_tmp[:,1:] = X\n",
    "        X = X_tmp\n",
    "    \n",
    "    m, p = X.shape\n",
    "\n",
    "    if theta is None:\n",
    "        theta = np.zeros((p,1))\n",
    "\n",
    "    if penalty_factors is None:\n",
    "        penalty_factors = np.ones((p,1))\n",
    "\n",
    "    lamdas = []\n",
    "    thetas = []\n",
    "    thetas_nat = []\n",
    "\n",
    "    for lamda in path:\n",
    "        if not warm_start:\n",
    "            theta = np.zeros((p,1))\n",
    "        sec_check_all_converged = False\n",
    "        active_set = np.arange(p)\n",
    "        active_set_converged = False\n",
    "        \n",
    "        for i in range(num_iters): \n",
    "            if (active_set.size != 0) and (not active_set_converged):\n",
    "                active_set_converged_check = np.full((len(active_set), ), False)\n",
    "                active_set_update = np.full((len(active_set), ), True)\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    \n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                        z += np.square(X[obs,j].item())\n",
    "                \n",
    "                    ###\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] =  tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "                \n",
    "            elif not sec_check_all_converged:\n",
    "                active_set = np.arange(p)\n",
    "                \n",
    "                active_set_converged_check = np.full((len(active_set), ), False)\n",
    "                active_set_update = np.full((len(active_set), ), True)\n",
    "\n",
    "                m, p = X.shape\n",
    "\n",
    "                for subindex, j in enumerate(active_set):\n",
    "                    w_j = penalty_factors[j].item()\n",
    "\n",
    "                    y_pred = X @ theta\n",
    "                    rho = 0.0\n",
    "                    z = 0.0\n",
    "\n",
    "                    for obs in range(m):\n",
    "                        rho += X[obs,j].item() * (y[obs].item() - y_pred[obs].item() + theta[j].item() * X[obs,j].item())\n",
    "                        z += np.square(X[obs,j].item())\n",
    "\n",
    "                    if intercept == True:  \n",
    "                        if j == 0:\n",
    "                            tmp = rho / z\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "                        else:\n",
    "                            tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                            if np.abs(tmp) < active_thresh:\n",
    "                                active_set_update[subindex] = False\n",
    "                            if np.abs(theta[j] - tmp) < thresh:\n",
    "                                active_set_converged_check[subindex] = True\n",
    "                            theta[j] =  tmp\n",
    "\n",
    "                    else:\n",
    "                        tmp = (1 / z) * soft_threshold_numba(rho, lamda, w_j)\n",
    "                        if np.abs(tmp) < active_thresh:\n",
    "                            active_set_update[subindex] = False\n",
    "                        if np.abs(theta[j] - tmp) < thresh:\n",
    "                            active_set_converged_check[subindex] = True\n",
    "                        theta[j] =  tmp\n",
    "\n",
    "                active_set_converged = np.all(active_set_converged_check)\n",
    "                active_set = active_set[active_set_update]\n",
    "                \n",
    "                if active_set_converged:\n",
    "                    sec_check_all_converged = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if intercept == False:\n",
    "            theta_tmp = theta.flatten() / x_std * y_std\n",
    "        if intercept == True:\n",
    "            theta_0 = (theta.flatten()[0] - np.sum((x_mean / x_std) * theta.flatten()[1:])) * y_std + y_mean\n",
    "            theta_betas = theta.flatten()[1:] / x_std * y_std\n",
    "            theta_tmp = np.ones((p,))\n",
    "            theta_tmp[1:] = theta_betas\n",
    "            theta_tmp[0] = theta_0\n",
    "        \n",
    "        lamdas.append(lamda)\n",
    "        thetas.append(theta)\n",
    "        thetas_nat.append(theta_tmp)\n",
    "\n",
    "    return lamdas,thetas,thetas_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "yellow-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_lamda_path_numba(X, y):\n",
    "    epsilon = 0.0001\n",
    "    K = 100\n",
    "    m, p = X.shape\n",
    "\n",
    "    y = y.reshape((m,1))\n",
    "    sx = X\n",
    "    sy = y\n",
    "\n",
    "    lambda_max = np.max(np.abs(np.sum(sx*sy, axis=0))) / m\n",
    "    lamda_path = np.exp(np.linspace(np.log(lambda_max), np.log(lambda_max*epsilon),np.int64(K)))\n",
    "\n",
    "    return lamda_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def adaptive_lasso(X, y):\n",
    "    res = lasso_numba(X = X, y = y, lamda_path = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "centered-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "committed-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lasso_numba(X = X, y = y, lamda_path = \"auto\", penalty_factors = np.abs(1 / reg.coef_.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "considered-orlando",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 150 is out of bounds for axis 0 with size 150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-91fd8c9f9f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_thresh_adaptive_lasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-7c02ab6d46c5>\u001b[0m in \u001b[0;36meps_thresh_adaptive_lasso\u001b[0;34m(X, y, penalty_factors, theta, lamda_path, num_iters, intercept, thresh)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtol_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0mw_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalty_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0mX_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 150 is out of bounds for axis 0 with size 150"
     ]
    }
   ],
   "source": [
    "res3 = eps_thresh_adaptive_lasso(X = X, y = y, lamda_path = \"auto\", penalty_factors = np.abs(1 / reg.coef_.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "peripheral-cleaners",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(1 / reg.coef_.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-vacation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
